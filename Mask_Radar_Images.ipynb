{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the best AD input parameters found by the Find_Events.ipynb script,\n",
    "load the results, find the corresponding start and end times for runoff events,\n",
    "query for any missing radar images, and batch mask the images to yield\n",
    "just the pixels representing each station catchment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1002\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1002\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import sys\n",
    "import math\n",
    "import utm\n",
    "import time\n",
    "\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from geopy import distance\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "import codecs\n",
    "\n",
    "from shapely.geometry import Point, shape, mapping, Polygon\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from bokeh.plotting import ColumnDataSource, output_notebook\n",
    "from bokeh.transform import factor_cmap, factor_mark\n",
    "from bokeh.palettes import Spectral3\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from radar_scrape import get_radar_img_urls, request_img_files\n",
    "from get_station_data import get_daily_runoff\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow\n",
    "\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.core import Dense \n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from radar_station_coords import radar_sites\n",
    "\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "DB_DIR = os.path.join(BASE_DIR, 'code/hydat_db')\n",
    "PROJECT_DIR = os.path.abspath('')\n",
    "# IMG_DIR = os.path.join(PROJECT_DIR, 'data/radar_img')\n",
    "RADAR_IMG_DIR = os.path.join(PROJECT_DIR, 'data/sorted_radar_images')\n",
    "\n",
    "RESULTS_DIR = os.path.join(PROJECT_DIR, 'data/AD_results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_radar_stn(row):\n",
    "    \"\"\" \n",
    "    Input the dict of all station distances,\n",
    "    Return the location code of the nearest radar station.\n",
    "    \"\"\"\n",
    "    radar_station_distances = row['radar_stn_distance_dict']\n",
    "    min_dist = min(radar_station_distances.items(), key=lambda x: x[1])\n",
    "    return min_dist[0]\n",
    "\n",
    "\n",
    "def find_closest_radar_stn_distance(row):\n",
    "    \"\"\" \n",
    "    Input the dict of all station distances,\n",
    "    Return the location code of the nearest radar station.\n",
    "    \"\"\"\n",
    "    radar_station_distances = row['radar_stn_distance_dict']\n",
    "    min_dist = min(radar_station_distances.items(), key=lambda x: x[1])\n",
    "    return min_dist[1]\n",
    "\n",
    "def calc_distance(wsc_row, station):\n",
    "    wsc_stn_coords = (wsc_row['Latitude'], wsc_row['Longitude'])\n",
    "    radar_coords = radar_sites[station]['lat_lon']\n",
    "    return distance.distance(radar_coords, wsc_stn_coords).km\n",
    "\n",
    "def calculate_radar_stn_distances(row):\n",
    "    distance_dict = {}\n",
    "    for site in radar_sites:\n",
    "        distance_dict[site] = calc_distance(row, site)\n",
    "    return distance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wsc_station_info_dataframe():\n",
    "    # import master station list\n",
    "    stations_df = pd.read_csv(DB_DIR + '/WSC_Stations_Master.csv')\n",
    "    # filter for stations that have concurrent record with the historical radar record\n",
    "    stations_df['RADAR_Overlap'] = stations_df['Year To'].astype(int) - 2007\n",
    "    stations_filtered = stations_df[stations_df['RADAR_Overlap'] > 0]\n",
    "    # filter for stations that are natural flow regimes\n",
    "    stations_filtered = stations_filtered[stations_filtered['Regulation'] == 'N']\n",
    "    stations_filtered.rename(columns={'Gross Drainage Area (km2)': 'DA'}, inplace=True)\n",
    "    # filter for stations in Alberta and British Columbia\n",
    "    stations_filtered = stations_filtered[(stations_filtered['Province'] == 'BC') | (stations_filtered['Province'] == 'AB')]\n",
    "    \n",
    "    # calculate distance to each radar station\n",
    "    stations_filtered['radar_stn_distance_dict'] = stations_filtered.apply(lambda row: calculate_radar_stn_distances(row), axis=1)    \n",
    "    stations_filtered['closest_radar_station'] = stations_filtered.apply(lambda row: find_closest_radar_stn(row), axis=1)\n",
    "    stations_filtered['radar_distance_km'] = stations_filtered.apply(lambda row: find_closest_radar_stn_distance(row), axis=1)\n",
    "    \n",
    "    # radar range is a 240km radius from the station\n",
    "    stations_filtered = stations_filtered[stations_filtered['radar_distance_km'] < 211]\n",
    "    stn_df = stations_filtered[np.isfinite(stations_filtered['DA'].astype(float))]\n",
    "    # filter for stations greater than 10 km^2 (too small for meaningful results)\n",
    "    stn_df = stn_df[stn_df['DA'].astype(float) >= 10]\n",
    "    # filter for stations smaller than 1000 km^2 (too large and complex)\n",
    "    stn_df = stn_df[stn_df['DA'].astype(float) < 1000].sort_values('DA')\n",
    "    df = stn_df[['Province', 'Station Number', 'Station Name', 'DA', \n",
    "                 'Elevation', 'Latitude', 'Longitude', 'RADAR_Overlap',\n",
    "                'closest_radar_station', 'radar_stn_distance_dict', 'radar_distance_km']]\n",
    "#     print('After filtering, there are {} candidate stations.'.format(len(stn_df)))\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_df = initialize_wsc_station_info_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 168 candidate stations.\n"
     ]
    }
   ],
   "source": [
    "print('After filtering, there are {} candidate stations.'.format(len(stn_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the results dataframe for the specified site\n",
    "results_folders = os.listdir(RESULTS_DIR)\n",
    "\n",
    "results_dict = {}\n",
    "# create a dictionary of results from all AD searches\n",
    "for f in results_folders:\n",
    "    folder_path = os.path.join(RESULTS_DIR, f)\n",
    "    all_sites = [e.split('_')[0] for e in os.listdir(folder_path)]\n",
    "    for site in all_sites:\n",
    "        if site in results_dict.keys():\n",
    "            old_results = results_dict[site]\n",
    "            new_results = pd.read_csv(os.path.join(folder_path, site + '_results.csv'))\n",
    "            results_dict[site] = pd.concat([old_results, new_results], sort=True)\n",
    "        else:            \n",
    "            results_dict[site] = pd.read_csv(os.path.join(folder_path, site + '_results.csv'))\n",
    "\n",
    "\n",
    "def get_best_result(site):\n",
    "    ad_df = pd.DataFrame(results_dict[site])\n",
    "    ad_df.drop(labels='Unnamed: 0', inplace=True, axis=1)\n",
    "    ad_df.sort_values('len_results', inplace=True, ascending=False)\n",
    "    return ad_df.iloc[0, :]\n",
    "\n",
    "all_sites = list(results_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all wsc_catchment data into its own dataframe\n",
    "gdb_path = os.path.join(DB_DIR, 'WSC_Basins.gdb.zip')\n",
    "all_layers = fiona.listlayers(gdb_path)\n",
    "all_layer_names = [e.split('_')[1].split('_')[0] for e in all_layers]\n",
    "filtered_layers = list(set(all_sites).intersection(all_layer_names))\n",
    "# some basins don't have geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 10 of 108: 05BM018\n",
      "layer 20 of 108: 05CE012\n",
      "layer 30 of 108: 08KA001\n",
      "layer 40 of 108: 08GA072\n",
      "layer 50 of 108: 05FA014\n",
      "layer 60 of 108: 05AB040\n",
      "layer 70 of 108: 08NN015\n",
      "layer 80 of 108: 05CA002\n",
      "layer 90 of 108: 08LF094\n",
      "layer 100 of 108: 08NE008\n"
     ]
    }
   ],
   "source": [
    "basin_df = gpd.GeoDataFrame()\n",
    "i = 1\n",
    "for layer in filtered_layers:\n",
    "    layer_label = 'EC_' + layer + '_1'\n",
    "    if i % 10 == 0:\n",
    "        print('layer {} of {}: {}'.format(i, len(filtered_layers), layer))\n",
    "    s = gpd.read_file(gdb_path, driver='FileGDB', layer=layer_label)\n",
    "    basin_df = basin_df.append(s, ignore_index=True)\n",
    "    i += 1\n",
    "    \n",
    "# print(basin_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_path = os.path.join(PROJECT_DIR, 'data/basin_geometry_data.geojson')\n",
    "basin_df.to_file(basin_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_df = stn_df[stn_df['Station Number'].isin(list(results_dict.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stn = stn_df['Station Number'].values[0]\n",
    "\n",
    "best_result = get_best_result(test_stn)\n",
    "\n",
    "test_stn_info = stn_df[stn_df['Station Number'] == test_stn]\n",
    "\n",
    "test_stn_name = test_stn_info['Station Name'].values[0]\n",
    "test_stn_da = test_stn_info['DA'].values[0]\n",
    "print('Station {} ({}) has a DA of {} km^2'.format(test_stn, test_stn_name, test_stn_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_daily_runoff(test_stn)\n",
    "df['Year'] = df.index.year\n",
    "df['Month'] = df.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by minimum radar date\n",
    "df = df[df.index > pd.to_datetime('2007-05-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Year'] > 2013].plot('Date', 'DAILY_FLOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peak_detection import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks(data, lag=7, threshold=500, influence=0.5):\n",
    "    # Settings (the ones below are examples: choose what is best for your data)\n",
    "#     lag = 5         # lag 5 for the smoothing functions\n",
    "#     threshold = 3.5  # 3.5 standard deviations for signal\n",
    "#     influence = 0.5  # between 0 and 1, where 1 is normal influence, 0.5 is half\n",
    "    # Initialize variables\n",
    "    signals = np.zeros(len(data))            # Initialize signal results\n",
    "    filteredY = np.empty(len(data))\n",
    "    filteredY[:lag] = data[:lag]             # Initialize filtered series\n",
    "    avgFilter = [0]                          # Initialize average filter\n",
    "    stdFilter = [0]                          # Initialize std. filter\n",
    "    avgFilter = {lag: np.mean(data[:lag])}      # Initialize first value\n",
    "    stdFilter = {lag: np.std(data[:lag])}     # Initialize first value\n",
    "    \n",
    "    for i in range(lag + 1, len(data)):\n",
    "        d = data[i]\n",
    "        \n",
    "        af = avgFilter[i-1]\n",
    "        sf = stdFilter[i-1]\n",
    "        \n",
    "        if abs(d - af) > threshold * sf:\n",
    "            if d > af:\n",
    "                signals[i] = 1                     # Positive signal\n",
    "            else:\n",
    "                signals[i] = -1                    # Negative signal\n",
    "\n",
    "            \n",
    "            filteredY[i] = influence*d + (1-influence)*filteredY[i-1]\n",
    "        else:\n",
    "            signals[i] = 0                        # No signal\n",
    "            filteredY[i] = 0\n",
    "        \n",
    "        \n",
    "        # Adjust the filters\n",
    "        avgFilter[i] = np.mean(filteredY[i-lag:i])\n",
    "        stdFilter[i] = np.std(filteredY[i-lag:i])\n",
    "        \n",
    "    return signals, filteredY\n",
    "\n",
    "n_test = 500\n",
    "\n",
    "dats = list(df['DAILY_FLOW'].to_numpy())\n",
    "sigs, f_dat = find_peaks(dats, influence=0.75, lag=7, threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show, output_notebook\n",
    "\n",
    "input_sig = df[['DAILY_FLOW']].copy()\n",
    "signal = np.array(sigs)\n",
    "input_sig['sig'] = signal.copy().astype(int)\n",
    "input_sig['f_sig'] = f_dat\n",
    "\n",
    "foo = input_sig[input_sig['sig'] == 1].copy()\n",
    "p = figure(plot_width=800, plot_height=400, x_axis_type='datetime')\n",
    "# add a circle renderer with a size, color, and alpha\n",
    "p.circle(foo.index, foo['DAILY_FLOW'], size=10, color=\"red\", \n",
    "         alpha=0.5, legend_label='{} pts'.format(len(foo)))\n",
    "# p.line(input_sig.index, input_sig['f_sig'], color='blue')\n",
    "p.line(input_sig.index, input_sig['DAILY_FLOW'], color='blue')\n",
    "# p.line()\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection Code\n",
    "\n",
    "Adapted from [Anomaly Detection ML Methods article](https://towardsdatascience.com/machine-learning-for-anomaly-detection-and-condition-monitoring-d4614e7de770)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_df(flow_df, stn_da):\n",
    "\n",
    "    lag_df = flow_df.copy()\n",
    "    \n",
    "    lag_df.rename(columns={'DAILY_FLOW': 'Q'}, inplace=True)\n",
    "\n",
    "    num_lags = int(np.ceil(stn_da / 100) + 5)\n",
    "\n",
    "    for i in range(1,num_lags):\n",
    "        lag_df['Q{}'.format(i)] = lag_df['Q'].shift(i)\n",
    "\n",
    "    lag_df.dropna(inplace=True)\n",
    "    \n",
    "    return lag_df, num_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_and_test_data(data, training_months, training_year):\n",
    "    time_range_check = (data.index.year == training_year) & (data.index.month.isin(training_months))\n",
    "    train_data = data[time_range_check]\n",
    "    # the test data is the entire dataset because we want to extract\n",
    "    # extreme events from the training year as well\n",
    "    test_data = data\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):\n",
    "    inv_covariance_matrix = inv_cov_matrix\n",
    "    vars_mean = mean_distr\n",
    "    diff = data - vars_mean\n",
    "    md = []\n",
    "    for i in range(len(diff)):\n",
    "        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))\n",
    "    return md\n",
    "\n",
    "def MD_detectOutliers(dist, extreme=False, verbose=False):\n",
    "    k = 3. if extreme else 2.\n",
    "    threshold = np.mean(dist) * k\n",
    "    outliers = []\n",
    "    for i in range(len(dist)):\n",
    "        if dist[i] >= threshold:\n",
    "            outliers.append(i)  # index of the outlier\n",
    "    return np.array(outliers)\n",
    "\n",
    "def MD_threshold(dist, extreme=False, verbose=False):\n",
    "    k = 3. if extreme else 2.\n",
    "    threshold = np.mean(dist) * k\n",
    "    return threshold\n",
    "\n",
    "def is_pos_def(A):\n",
    "    if np.allclose(A, A.T):\n",
    "        try:\n",
    "            np.linalg.cholesky(A)\n",
    "            return True\n",
    "        except np.linalg.LinAlgError:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_runoff_dataframe(test_stn):\n",
    "    runoff_df = get_daily_runoff(test_stn)\n",
    "    runoff_df['Q'] = runoff_df['DAILY_FLOW']\n",
    "    runoff_df = runoff_df[['Q']]\n",
    "    \n",
    "    # filter by minimum radar date\n",
    "    runoff_df = runoff_df[runoff_df.index > pd.to_datetime('2007-05-31')]\n",
    "  \n",
    "    return runoff_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_PCA(X_train, X_test, n_components):\n",
    "    \n",
    "    for n_components_kept in range(2, n_components + 1):\n",
    "\n",
    "        pca = PCA(n_components=n_components_kept, svd_solver= 'full')\n",
    "        X_train_PCA = pca.fit_transform(X_train)\n",
    "        X_train_PCA = pd.DataFrame(X_train_PCA)\n",
    "        X_train_PCA.index = X_train.index\n",
    "\n",
    "        X_test_PCA = pca.transform(X_test)\n",
    "        X_test_PCA = pd.DataFrame(X_test_PCA)\n",
    "        X_test_PCA.index = X_test.index\n",
    "\n",
    "        var_expl = 100*np.sum(pca.explained_variance_ratio_)\n",
    "        if var_expl >= 90:\n",
    "#             print('var > 0.9 in {} components'.format(n_components_kept))\n",
    "            return X_train_PCA, X_test_PCA, var_expl, n_components_kept\n",
    "#     print('var < 0.9 in {} components'.format(n_components_kept))\n",
    "    return X_train_PCA, X_test_PCA, var_expl, n_components_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_from_annual_distribution(df):\n",
    "    annual_dist = df.groupby(df.index.month).mean()\n",
    "#     print(annual_dist)\n",
    "    annual_dist['rank'] = annual_dist['Q'].rank(ascending=False)\n",
    "    annual_dist['b'] = 1 - annual_dist['rank'] / float(len(annual_dist) - 1)\n",
    "    june_val = annual_dist[annual_dist.index == 6]['b'].values[0]\n",
    "    if june_val > 0.8:\n",
    "        return 7\n",
    "    else:\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_data):\n",
    "    tstart = time.time()\n",
    "    training_months = input_data['months']\n",
    "    training_year = input_data['year']\n",
    "    wsc_station_num = input_data['wsc_stn']\n",
    "    training_sample_size = input_data['n_sample']\n",
    "    stn_da = input_data['stn_da']\n",
    "    closest_radar_stn = input_data['radar_stn']\n",
    "    \n",
    "    lag_df = input_data['lag_df']\n",
    "    num_lags = input_data['num_lags']\n",
    "    runoff_df = input_data['runoff_df']\n",
    "    \n",
    "    dataset_train, dataset_test = split_train_and_test_data(lag_df, training_months, training_year)\n",
    "    \n",
    "    training_set_len = len(dataset_train)\n",
    "    \n",
    "    if len(dataset_train) < 25:\n",
    "#         print('exited because dataset_train is too small')\n",
    "#         print(dataset_train)\n",
    "        return pd.DataFrame([]), 0\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(dataset_train), \n",
    "                                  columns=dataset_train.columns, \n",
    "                                  index=dataset_train.index)\n",
    "    # Random shuffle training data\n",
    "    X_train.sample(frac=1)\n",
    "\n",
    "    X_test = pd.DataFrame(scaler.transform(dataset_test), \n",
    "                                 columns=dataset_test.columns, \n",
    "                                 index=dataset_test.index)\n",
    "    t1 = time.time()\n",
    "    \n",
    "   \n",
    "    X_train_PCA, X_test_PCA, var_expl, n_components = do_PCA(X_train, X_test, num_lags)\n",
    "    \n",
    "    data_train = np.array(X_train_PCA.values)\n",
    "    data_test = np.array(X_test_PCA.values)\n",
    "    t2 = time.time()\n",
    "#     print('time to end of PCA = {:.4f}'.format(t2-tstart))\n",
    "\n",
    "    \n",
    "    def cov_matrix(data, verbose=False):\n",
    "        covariance_matrix = np.cov(data, rowvar=False)\n",
    "        if is_pos_def(covariance_matrix):\n",
    "            inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n",
    "            if is_pos_def(inv_covariance_matrix):\n",
    "                return True, covariance_matrix, inv_covariance_matrix\n",
    "            else:\n",
    "                print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n",
    "                return False, None, None\n",
    "        else:\n",
    "#             print(\"Error: Covariance Matrix is not positive definite!\")\n",
    "            return False, None, None\n",
    "\n",
    "               \n",
    "    cov_test, cov_matrix, inv_cov_matrix = cov_matrix(data_train)\n",
    "    \n",
    "    if cov_test == False:\n",
    "        return pd.DataFrame([]), 0\n",
    "\n",
    "    mean_distr = data_train.mean(axis=0)\n",
    "\n",
    "    dist_test = MahalanobisDist(inv_cov_matrix, mean_distr, data_test, verbose=False)\n",
    "    dist_train = MahalanobisDist(inv_cov_matrix, mean_distr, data_train, verbose=False)\n",
    "    threshold = MD_threshold(dist_train, extreme = True)\n",
    "    \n",
    "    anomaly_train = pd.DataFrame()\n",
    "    anomaly_train['Mob dist']= dist_train\n",
    "    anomaly_train['Thresh'] = threshold\n",
    "    # If Mob dist above threshold: Flag as anomaly\n",
    "    anomaly_train['Anomaly'] = anomaly_train['Mob dist'] > anomaly_train['Thresh']\n",
    "    anomaly_train.index = X_train_PCA.index\n",
    "    anomaly = pd.DataFrame()\n",
    "    anomaly['Mob dist']= dist_test\n",
    "    anomaly['Thresh'] = threshold\n",
    "    anomaly['num_components_kept'] = n_components\n",
    "    # If Mob dist above threshold: Flag as anomaly\n",
    "    anomaly['Anomaly'] = anomaly['Mob dist'] > anomaly['Thresh']\n",
    "    anomaly.index = X_test_PCA.index\n",
    "    anomaly.head()\n",
    "    \n",
    "    anomaly_alldata = pd.concat([anomaly_train, anomaly], sort=True)\n",
    "    \n",
    "    event_times = np.where(anomaly_alldata['Anomaly'].values[:-1] != anomaly_alldata['Anomaly'].values[1:])[0]\n",
    "    events = pd.merge(lag_df, anomaly_alldata.iloc[event_times,:], how='inner', \n",
    "                      left_index=True, right_index=True)\n",
    "\n",
    "    events = events.loc[~events.index.duplicated(keep='first')]\n",
    "    \n",
    "    if len(events) < 5:\n",
    "#         print('exited because len(events) < 5')\n",
    "        return pd.DataFrame([]), n_components\n",
    "    elif events.iloc[0]['Anomaly'] == True:\n",
    "        events = events.iloc[1:]\n",
    "        \n",
    "    # create a column of time difference between events in days\n",
    "    events['dt_days'] = events.index.to_series().diff(1)    \n",
    "\n",
    "    a = time.time()\n",
    "\n",
    "    last_event_end = False\n",
    "\n",
    "    new_events = pd.DataFrame()\n",
    "\n",
    "    # iterate through the detected event pairs \n",
    "    for i in np.arange(0, len(events) - 1, 2):\n",
    "        \n",
    "        # parse a single event pair\n",
    "        this_event = events.iloc[i:i+2]\n",
    "        \n",
    "        check_sign_switch = this_event['Anomaly'].values[0] != this_event['Anomaly'].values[1]\n",
    "        taa = time.time()\n",
    "        concurrent_wsc = lag_df[(lag_df.index >= this_event.index.values[0]) & (lag_df.index <= this_event.index.values[1])][['Q']]\n",
    "        peak_in_middle = check_peak_in_middle(this_event, concurrent_wsc)\n",
    "\n",
    "\n",
    "        if (check_sign_switch) & (peak_in_middle):\n",
    "            \n",
    "            \n",
    "            \n",
    "            # get the start date\n",
    "            this_event_start = pd.to_datetime(this_event[this_event['Anomaly'] == False].index.values[0])\n",
    "            # get the end date\n",
    "            this_event_end = pd.to_datetime(this_event[this_event['Anomaly'] == True].index.values[0])\n",
    "            tloops = time.time()\n",
    "            adjusted_start_date = pd.to_datetime(adjust_edge_date(this_event_start, lag_df[['Q']], 'start', stn_da))\n",
    "            adjusted_end_date = pd.to_datetime(adjust_edge_date(this_event_end, lag_df[['Q']], 'end', stn_da))\n",
    "            \n",
    "            lag_df_start = pd.to_datetime(lag_df.index.values[0])\n",
    "            \n",
    "            tin = time.time()\n",
    "#             print('asd {:.3f}'.format(tin - tloops))\n",
    "            \n",
    "            # check if the adjusted start date predates the record\n",
    "            if lag_df_start > adjusted_start_date:\n",
    "                adjusted_start_date = lag_df_start\n",
    "#                 print('this was adjusted')\n",
    "            \n",
    "            if last_event_end is not False:\n",
    "                # find if the start date is on the rising limb - adjust if so\n",
    "\n",
    "                if adjusted_start_date < last_event_end:\n",
    "                    adjusted_start_date = last_event_end + pd.DateOffset(1)\n",
    "                    \n",
    "            new_event_start = lag_df[lag_df.index == adjusted_start_date][['Q']]\n",
    "            new_event_end = lag_df[lag_df.index == adjusted_end_date][['Q']] \n",
    "\n",
    "            new_event_start['timing'] = 'start'\n",
    "            new_event_end['timing'] = 'end'\n",
    "            \n",
    "            start_month_limit = get_start_from_annual_distribution(runoff_df)            \n",
    "            \n",
    "            if stn_da < 100:\n",
    "                max_days = 4\n",
    "            elif stn_da < 500:\n",
    "                max_days = 6\n",
    "            else:\n",
    "                max_days = 14\n",
    "                \n",
    "            if len(new_event_start) == 0:\n",
    "                new_event_start = lag_df[lag_df.index == this_event_start][['Q']]\n",
    "            if len(new_event_end) == 0:\n",
    "                new_event_end = lag_df[lag_df.index == this_event_end][['Q']]\n",
    "            \n",
    "            min_time_check = (new_event_end.index - new_event_start.index).days > 1\n",
    "            max_time_check = (new_event_end.index - new_event_start.index).days <= max_days\n",
    "            start_month = new_event_start.index.month\n",
    "            \n",
    "            end_month = new_event_end.index.month\n",
    "            season_check = (start_month > 5) & (start_month < 11) & (end_month <= 11)\n",
    "\n",
    "            if (min_time_check) & (max_time_check) & (season_check):\n",
    "                new_events = new_events.append(new_event_start, sort=True)\n",
    "                new_events = new_events.append(new_event_end, sort=True)\n",
    "\n",
    "            last_event_end = pd.to_datetime(this_event_end)\n",
    "            \n",
    "\n",
    "    new_events.sort_index(inplace=True)    \n",
    "\n",
    "    new_events['dt_days'] = new_events.index.to_series().diff(1)\n",
    "    new_events['wsc_station'] = wsc_station_num\n",
    "    new_events['training_year'] = training_year\n",
    "    new_events['training_months'] = str(training_months)# for e in new_events]\n",
    "    new_events['training_set_len'] = training_set_len\n",
    "    new_events['m_threshold'] = threshold\n",
    "    new_events['var_explained'] = var_expl\n",
    "    new_events['n_components'] = n_components\n",
    "    new_events['num_lags'] = num_lags\n",
    "    new_events['radar_stn'] = closest_radar_stn\n",
    "                \n",
    "    return new_events, n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_edge_date(initial_date, data, direction, stn_da):\n",
    "    \"\"\"\n",
    "    If the start flow is on a rising limb, adjust the start to the start of the runoff event.\n",
    "    \"\"\"\n",
    "    initial_val = data[data.index == initial_date]['Q']\n",
    "\n",
    "    \n",
    "    if direction == 'start':\n",
    "        search_criteria = (data.index >= initial_date - pd.Timedelta('7 days')) & (data.index <= initial_date)\n",
    "        search_direction = 1\n",
    "    elif direction == 'end':\n",
    "        search_criteria = (data.index <= initial_date + pd.Timedelta('3 days')) & (data.index >= initial_date)\n",
    "        search_direction = 1\n",
    "\n",
    "        \n",
    "    extended_week_vals = data[search_criteria][['Q']]\n",
    "    extended_week_vals['diff'] = extended_week_vals.diff(periods=search_direction)\n",
    "    extended_week_vals['pct_change'] = 100 * extended_week_vals['diff'] / extended_week_vals['Q']\n",
    "\n",
    "    if direction == 'start':\n",
    "        try:\n",
    "            change_date = pd.to_datetime(extended_week_vals[['Q']].idxmin().values[0])\n",
    "            change_point_date = change_date - pd.DateOffset(1)\n",
    "            adjusted_date = change_point_date\n",
    "            \n",
    "        except ValueError as err:\n",
    "            adjusted_date = initial_date\n",
    "\n",
    "    elif direction == 'end':\n",
    "        try:\n",
    "            adjusted_date = pd.to_datetime(extended_week_vals[['diff']].idxmin().values[0])\n",
    "        except ValueError as err:\n",
    "            print('print error in adjusting event end date', err)\n",
    "            adjusted_date = initial_date\n",
    "            \n",
    "    return adjusted_date\n",
    "\n",
    "\n",
    "def check_peak_in_middle(event, data):\n",
    "    \"\"\"\n",
    "    Ensure there is a peak between the start and end points\n",
    "    so we aren't targeting a non-runoff event.\n",
    "    \"\"\"\n",
    "    start_time = event.index.values[0] \n",
    "    end_time = event.index.values[-1]\n",
    "    max_time = data[data['Q'] == data['Q'].max()].index.values[0]\n",
    "    if (max_time == start_time) | (max_time == end_time):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def get_all_combinations(months, years):\n",
    "    month_combos = [list(itertools.combinations(months, n)) for n in list(range(1, 13))]\n",
    "    flat_combos =  [item for sublist in month_combos for item in sublist]\n",
    "    return np.asarray(list(itertools.product(flat_combos, years)))\n",
    "\n",
    "def calc_softmax(X):\n",
    "    return np.exp(X) / np.sum(np.exp(X))\n",
    "\n",
    "def filter_input_data(data):\n",
    "    filtered = []\n",
    "    for d in data:\n",
    "        months = list(d[0])\n",
    "        common_months = [m for m in months if m in [7, 8, 9, 10]]\n",
    "        if (len(months) == 1) & (months[0] not in [12, 1, 2, 3]):\n",
    "            filtered.append(list(d))\n",
    "        elif (len(months) > 1) & (len(common_months) > 0):\n",
    "            filtered.append(list(d))\n",
    "    return filtered\n",
    "\n",
    "def run_AD_training(wsc_station_num, training_sample_size=5):\n",
    "    \n",
    "    radar_stn = stn_df[stn_df['Station Number'] == wsc_station_num]['closest_radar_station'].values[0]\n",
    "    stn_da = stn_df[stn_df['Station Number'] == wsc_station_num]['DA'].values[0]\n",
    "    \n",
    "    lag_df, closest_radar_stn, runoff_df, num_lags = initialize_input_data(wsc_station_num)\n",
    "    \n",
    "#     runoff_df = initialize_runoff_dataframe(wsc_station_num)  \n",
    "    \n",
    "    training_months = list(set(runoff_df.index.month))\n",
    "    training_years = list(set(runoff_df.index.year))\n",
    "\n",
    "    all_combinations = get_all_combinations(training_months, training_years)\n",
    "    \n",
    "    filtered_combinations = np.array(filter_input_data(all_combinations))\n",
    "        \n",
    "    weights = calc_softmax([(13.0 - len(c[0]))*3.5 for c in filtered_combinations])\n",
    "    \n",
    "    # a complete search is intractable, so sample n permutations without replacement\n",
    "    rand_ints = np.random.choice(range(len(filtered_combinations)), training_sample_size, \n",
    "                                 replace=False, p=weights)\n",
    "    \n",
    "    initial_pop = [filtered_combinations[i] for i in rand_ints]\n",
    "    \n",
    "\n",
    "    input_array = []\n",
    "    for combo in initial_pop:\n",
    "        input_data = {'year': combo[1],\n",
    "                     'months': combo[0],\n",
    "                     'n_sample': training_sample_size,\n",
    "                     'wsc_stn': wsc_station_num,\n",
    "                     'stn_da': stn_da,\n",
    "                     'radar_stn': radar_stn,\n",
    "                      'lag_df': lag_df,\n",
    "                      'num_lags': num_lags,\n",
    "                      'runoff_df': runoff_df\n",
    "                     }\n",
    "        input_array.append(input_data)\n",
    "        \n",
    "    results = []\n",
    "    for input_dat in input_array:\n",
    "        result, n_components = train_model(input_dat)\n",
    "        results.append((input_dat, n_components, result))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_input_data(wsc_stn_num):\n",
    "        \n",
    "    t0 = time.time()\n",
    "    stn_df = initialize_wsc_station_info_dataframe()\n",
    "\n",
    "    test_stn_info = stn_df[stn_df['Station Number'] == wsc_stn_num]\n",
    "    stn_da = test_stn_info['DA'].values[0]\n",
    "    wsc_stn_name = test_stn_info['Station Name'].values[0]\n",
    "    closest_radar_stn = test_stn_info['closest_radar_station'].values[0]\n",
    "#     print('{} ({}) has a DA of {} km^2'.format(wsc_stn_name, wsc_stn_num, stn_da))\n",
    "    \n",
    "    runoff_df = initialize_runoff_dataframe(wsc_stn_num)    \n",
    "    lag_df, num_lags = create_lag_df(runoff_df, stn_da) \n",
    "    \n",
    "    \n",
    "    candidate_stations = stn_df['Station Number'].values\n",
    "    \n",
    "    return lag_df, closest_radar_stn, runoff_df, num_lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to test a single station and view AD results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_AD_from_results(test_stn):\n",
    "\n",
    "    test_flow_df = get_daily_runoff(test_stn)\n",
    "    test_flow_df = test_flow_df[test_flow_df.index.year >= 2007]\n",
    "    test_flow_df.rename(columns={'DAILY_FLOW': 'Q'}, inplace=True)\n",
    "    test_flow_df = test_flow_df[['Q']]\n",
    "    \n",
    "    radar_stn = stn_df[stn_df['Station Number'] == test_stn]['closest_radar_station'].values[0]\n",
    "    stn_da = stn_df[stn_df['Station Number'] == test_stn]['DA'].values[0]\n",
    "    \n",
    "    lag_df, closest_radar_stn, runoff_df, num_lags = initialize_input_data(test_stn)\n",
    "    \n",
    "    best_training_params = get_best_result(test_stn)\n",
    "\n",
    "    months = [m for m in best_training_params[0][1:-1].split(',') if len(m) > 0]\n",
    "    train_months = [int(m) for m in months]\n",
    "       \n",
    "    input_data = {'year': best_training_params[1],\n",
    "             'months': train_months,\n",
    "             'n_sample': best_training_params[4],\n",
    "             'wsc_stn': test_stn,\n",
    "             'stn_da': stn_da,\n",
    "             'radar_stn': radar_stn,\n",
    "              'lag_df': lag_df,\n",
    "              'num_lags': num_lags,\n",
    "              'runoff_df': runoff_df\n",
    "            }\n",
    "        \n",
    "    best_events, n_components = train_model(input_data)\n",
    "\n",
    "    return best_events, test_flow_df, closest_radar_stn\n",
    "\n",
    "test_stn = '05CA012'\n",
    "# print(best_training_params)\n",
    "best_events, test_flow_df, closest_radar_stn = run_AD_from_results(test_stn)\n",
    "print(best_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=800, plot_height=400, x_axis_type='datetime')\n",
    "\n",
    "e1 = best_events[best_events['timing'] == 'start']\n",
    "e2 = best_events[best_events['timing'] == 'end']\n",
    "\n",
    "p.circle(e1.index, e1['Q'], color='red', alpha=0.5, size=10, legend_label='start')\n",
    "p.circle(e2.index, e2['Q'], color='blue', alpha=0.5, size=10, legend_label='end')\n",
    "\n",
    "# p.line(input_sig.index, input_sig['f_sig'], color='blue')\n",
    "p.line(test_flow_df.index, test_flow_df['Q'], color='blue')\n",
    "# p.line()\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid plot of individual events\n",
    "\n",
    "plots = []\n",
    "\n",
    "for i in np.arange(0, len(best_events) - 1, 2):\n",
    "    \n",
    "    # parse a single event pair\n",
    "    this_event = best_events.iloc[i:i+2]\n",
    "    \n",
    "    s1 = figure(background_fill_color=\"#fafafa\", x_axis_type='datetime')\n",
    "    \n",
    "    s1.circle(this_event.index, this_event['Q'], \n",
    "              size=12, alpha=0.8, color=\"red\")#, legend_label='{estimated endpoints}')\n",
    "    s1.xaxis.major_label_orientation = math.pi/2\n",
    "    this_start = pd.to_datetime(this_event.index.values[0])\n",
    "    this_end = pd.to_datetime(this_event.index.values[1])\n",
    "    this_dat = test_flow_df[(test_flow_df.index >= this_start) & (test_flow_df.index <= this_end)][['Q']]\n",
    "    \n",
    "    if (this_end.month < 12) & (this_start.month > 5):\n",
    "        year = this_event.index.year.values[0]\n",
    "        month = this_event.index.month.values[0]\n",
    "        day = this_event.index.day.values[0]\n",
    "        date = '{}-{}-{}'.format(year, month, day)\n",
    "        s1.line(this_dat.index, this_dat['Q'], color='blue')\n",
    "        plots.append(s1)\n",
    "\n",
    "print('there are {} plots'.format(len(plots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(plots) < 6:\n",
    "#     grid = gridplot(plots, plot_width=150, plot_height=150)\n",
    "# else:\n",
    "n_cols = 8\n",
    "n_rows = int(np.ceil(len(plots) / n_cols))\n",
    "\n",
    "\n",
    "g = []\n",
    "for i in range(0, len(plots), n_cols):\n",
    "    g += [plots[i:i+n_cols]]\n",
    "grid = gridplot(g, plot_width=150, plot_height=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to batch process all station events and aquire corresponding radar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_event_pairs(best_events):\n",
    "    event_pairs = []\n",
    "    for i in np.arange(0, len(best_events) - 1, 2):\n",
    "        # parse a single event pair\n",
    "        this_event = best_events.iloc[i:i+2]\n",
    "        date_pair = [e.astype(str).replace('T', ' ').split('.')[0].split(' ')[0] for e in this_event.index.values]\n",
    "        this_start = pd.to_datetime(this_event.index.values[0])\n",
    "        this_month = this_start.month\n",
    "        if (this_month > 5) & (this_month <= 11) & (this_start > pd.to_datetime('2007-01-01')):\n",
    "            event_pairs.append(date_pair)\n",
    "    return event_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for stn in all_sites: \n",
    "    i += 1\n",
    "    print(stn)\n",
    "    best_events, test_flow_df, closest_radar_stn = run_AD_from_results(stn)\n",
    "    event_pairs = get_all_event_pairs(best_events)\n",
    "    all_img_urls = get_radar_img_urls(event_pairs, closest_radar_stn)\n",
    "    n_images = len(all_img_urls)\n",
    "    if n_images > 0:\n",
    "        print('{} of {}: {} image files to query for {} with {} events identified.'.format(i, len(all_sites), \n",
    "                                                                                           len(all_img_urls), stn, \n",
    "                                                                                           len(event_pairs)))\n",
    "    time0 = time.time()    \n",
    "    request_img_files(all_img_urls, stn, closest_radar_stn)\n",
    "    time1 = time.time()\n",
    "    print('    {:.0f}s to query all images'.format(time1 - time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Radar, Mask with Catchment, and Clip all Retrieved Images\n",
    "\n",
    "First, make sure basin geometry is available before requesting all images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "\n",
    "\n",
    "## Further Work\n",
    "\n",
    "Find all the stations that have radar overlap.  \n",
    "\n",
    "Find a bunch of concurrent runoff events, and estimate the error based on the overlapping radar info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ubc_polygon():\n",
    "    points = [-123.1876189229259,49.27349631820059, -123.2168376964195,49.27886813665509,\n",
    "              -123.250951627253,49.28123248845857, -123.2686880871375,49.26628206996413, \n",
    "              -123.2577985144227,49.25107796072373, -123.2296736741512,49.23345938143058, \n",
    "              -123.2086850019333,49.22474925352834, -123.1911799345363,49.22020024319045, \n",
    "              -123.1852881427934,49.2196036807795, -123.1876189229259,49.27349631820059, ]\n",
    "    lat_list = []\n",
    "    lon_list = []\n",
    "    for i in range(0, len(points), 2):\n",
    "        lat_list.append(points[i])\n",
    "        lon_list.append(points[i + 1])\n",
    "#     gdb_path = os.path.join(PROJECT_DIR, path)\n",
    "#     data = gpd.read_file(gdb_path, driver='kmz')\n",
    "#     return data.geometry\n",
    "    minx = min(lat_list)\n",
    "    maxx = max(lat_list)\n",
    "    miny = min(lon_list)\n",
    "    maxy = max(lon_list)\n",
    "    bbox = pd.DataFrame({'minx': [minx], 'miny': [miny],\n",
    "                        'maxx': [maxx], 'maxy': [maxy]})\n",
    "    return Polygon(zip(lat_list, lon_list)), bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.speedups\n",
    "shapely.speedups.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_coordinates(closest_stn):\n",
    "    # note that the pixel coordinates are projected to NAD83\n",
    "    # coordinate system by assuming each pixel is 1 / 111.32 \n",
    "    # degree seconds, so the error propagates from the centre\n",
    "    # to 0.01 * 240 = 2.4 metres at the edges of the image\n",
    "    img_coord_path = 'data/radar_img_pixel_coords'\n",
    "    img_coord_file = [f for f in os.listdir(img_coord_path) if closest_stn in f][0]\n",
    "    geo_df = gpd.read_file(os.path.join(img_coord_path, img_coord_file))\n",
    "    geo_df = geo_df.to_crs('EPSG:4269')\n",
    "#     return pd.read_json(os.path.join(img_coord_path, img_coord_file))\n",
    "    return geo_df\n",
    "\n",
    "\n",
    "def get_img_mask(closest_stn, basin_geom, wsc_stn):\n",
    "    # take in the basin geometry and its corresponding radar station\n",
    "    # return a 480x480 boolean matrix where True represents\n",
    "    # pixels that fall within the basin polygon\n",
    "    # note that the points are stored in (lat, lon) tuples\n",
    "    # which corresponds to y, x\n",
    "    basin_geom = basin_geom.to_crs('EPSG:4326')\n",
    "    print('basin geom:', basin_geom.crs)\n",
    "#     basin_geom_data = basin_data.geometry\n",
    "    print(basin_geom.bounds)\n",
    "#     print(basin_geom_data)\n",
    "    \n",
    "    mask_folder = 'data/wsc_stn_basin_masks'\n",
    "    mask_path = os.path.join(PROJECT_DIR, mask_folder)\n",
    "    mask_filename = wsc_stn + '.json'\n",
    "    existing_masks = os.listdir(mask_path)\n",
    "\n",
    "    if mask_filename in existing_masks:\n",
    "        return pd.read_json(os.path.join(mask_path, mask_filename)).to_numpy()\n",
    "    else:\n",
    "        print('No image mask exists.  Try creating one...')\n",
    "        geo_df = get_pixel_coordinates(closest_stn)#.to_numpy().flatten()\n",
    "\n",
    "    # match the crs of the basin polygon\n",
    "    geo_df = geo_df.to_crs('EPSG:4326')\n",
    "    print(basin_geom.geometry)\n",
    "#     geo_df = geo_df.to_crs('EPSG:4269')\n",
    "    \n",
    "#     print('geo df bounds:')\n",
    "#     print(geo_df.total_bounds)\n",
    "#     print(geo_df.crs)\n",
    "#     print('catchment geom bounds:')\n",
    "#     print(basin_geom.bounds)\n",
    "#     print('')\n",
    "\n",
    "    pip_mask = geo_df.within(basin_geom.geometry)\n",
    "    \n",
    "    reshaped_mask = np.array(pip_mask).reshape(480, 480)\n",
    "\n",
    "    img_mask_df = pd.DataFrame(reshaped_mask)\n",
    "\n",
    "    img_mask_df.to_json(os.path.join(mask_path, mask_filename), orient='columns')\n",
    "    \n",
    "    print('saved json')\n",
    "\n",
    "\n",
    "def bbox2(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    ymin, ymax = np.where(rows)[0][[0, -1]]\n",
    "    xmin, xmax = np.where(cols)[0][[0, -1]]\n",
    "    return img[ymin:ymax+1, xmin:xmax+1]\n",
    "\n",
    "\n",
    "@jit\n",
    "def mask_image(img_array, mask):\n",
    "    mask_idx = np.where(mask==0)\n",
    "    rows = mask_idx[0]\n",
    "    cols = mask_idx[1]\n",
    "    filtered_img = np.zeros((480, 480, 3))\n",
    "    for n in range(480):\n",
    "        for m in range(480):\n",
    "            if mask[n, m] == 0:\n",
    "                filtered_img[n, m] = 0\n",
    "            else:\n",
    "                filtered_img[n, m] = img_array[n, m]\n",
    "    return filtered_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_images_and_save(gif_images, mask, test_stn, best_radar_stn):\n",
    "#     print(best_radar_stn)\n",
    "    origin_path = os.path.join(RADAR_IMG_DIR, best_radar_stn)\n",
    "    save_path = os.path.join(PROJECT_DIR, 'data/masked_img/{}'.format(test_stn))\n",
    "    for t_img in gif_images:\n",
    "        new_filename = t_img.split('.')[0] + '_crp.gif'\n",
    "        time0 = time.time()      \n",
    "        origin_full_path = os.path.join(origin_path, t_img)\n",
    "        new_file_path = os.path.join(save_path, new_filename)\n",
    "        # there are a large number of empty images that get returned\n",
    "        # from image retrievel.  DON'T DELETE THEM or you will \n",
    "        # request them many times over in the AD function.\n",
    "        # Instead, filter them out here.\n",
    "        if os.path.getsize(origin_full_path) > 100:\n",
    "            gif_img = Image.open(origin_full_path).convert('RGB')\n",
    "            img_array = np.asarray(gif_img)[:,:480].astype(np.uint8)        \n",
    "            filtered_img = mask_image(img_array, mask)\n",
    "            time1 = time.time()\n",
    "            try:\n",
    "                cropped_array = bbox2(filtered_img)\n",
    "                cropped_img = Image.fromarray(cropped_array.astype(np.uint8))\n",
    "                cropped_img.save(os.path.join(save_path, new_filename))\n",
    "            except Exception as e:\n",
    "                print('Something broke in the image mask process for {}'.format(test_stn))\n",
    "                print(e)\n",
    "                break\n",
    "\n",
    "\n",
    "def get_event_times(df):\n",
    "    event_pairs = []\n",
    "    for i in np.arange(0, len(df) - 1, 2):    \n",
    "        # parse a single event pair\n",
    "        this_event = df.iloc[i:i+2]\n",
    "        date_pair = [e.astype(str).replace('T', ' ').split('.')[0].split(' ')[0] for e in this_event.index.values]\n",
    "        this_start = pd.to_datetime(this_event.index.values[0])\n",
    "        this_month = this_start.month\n",
    "        if (this_month > 5) & (this_month <= 11) & (this_start > pd.to_datetime('2007-01-01')):\n",
    "            event_pairs.append(date_pair)\n",
    "    return event_pairs\n",
    "    \n",
    "    \n",
    "def get_possible_filenames_from_event_time(event_pair):\n",
    "    filenames = []\n",
    "    for p in event_pair:\n",
    "        start, end = p[0], p[1]\n",
    "        datetime_range = [str(pd.to_datetime(e).strftime('%Y%m%d%H%M')) + '.gif' for e in pd.date_range(pd.to_datetime(start), \n",
    "                                       pd.to_datetime(end),\n",
    "                                      freq='1H').values]\n",
    "        filenames += datetime_range\n",
    "    return set(filenames)\n",
    "\n",
    "\n",
    "def get_radar_images_for_events(wsc_stn, radar_stn, best_events):\n",
    "    event_df = best_events\n",
    "    event_pairs = get_event_times(event_df)\n",
    "    \n",
    "    files_to_search = get_possible_filenames_from_event_time(event_pairs)\n",
    "    \n",
    "    existing_files = os.listdir(os.path.join(RADAR_IMG_DIR, radar_stn))\n",
    "    \n",
    "#     new_files = np.setdiff1d(files_to_search, existing_files)\n",
    "    new_files = files_to_search.intersection(existing_files)\n",
    "    return new_files\n",
    "\n",
    "\n",
    "def get_gif_images(test_stn, closest_radar_stn, best_events):\n",
    "    gif_path = 'data/sorted_radar_images/{}'.format(closest_radar_stn)\n",
    "    save_path = 'data/masked_img/{}'.format(test_stn)\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    return list(get_radar_images_for_events(test_stn, closest_radar_stn, best_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basin_geometry(test_stn):\n",
    "    # original WSC basin polygon is EPSG: 4269 (NAD83)\n",
    "    # WGS 84 is EPSG: 4326\n",
    "    basin_data = basin_df[basin_df['Station'] == test_stn]\n",
    "    return basin_data\n",
    "\n",
    "def calculate_distance(row):\n",
    "    wsc_stn_coords = [row['Latitude'], row['Longitude']]\n",
    "    radar = row['Closest_radar']\n",
    "    \n",
    "    radar_coords = radar_stations[radar]['lat_lon']\n",
    "    xs = [wsc_stn_coords[1], radar_coords[1]]\n",
    "    ys = [wsc_stn_coords[0], radar_coords[0]]\n",
    "\n",
    "    geom = [Point(xy) for xy in zip(xs, ys)]\n",
    "    gdf = gpd.GeoDataFrame(geometry=geom, crs='epsg:4269')\n",
    "    gdf.to_crs(epsg=3310, inplace=True)\n",
    "    # calculate distance in kilometers\n",
    "    l = gdf.distance(gdf.shift()).values[-1] / 1000\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(wsc_stn):\n",
    "    \n",
    "    best_events, test_flow_df, closest_radar_stn = run_AD_from_results(wsc_stn)\n",
    "    event_pairs = get_all_event_pairs(best_events)\n",
    "    \n",
    "#     print('closest radar = ', closest_radar_stn)\n",
    "    \n",
    "    stn_da = stn_df[stn_df['Station Number'] == wsc_stn]['DA'].values[0]\n",
    "    \n",
    "    try:\n",
    "        basin_geom = get_basin_geometry(wsc_stn)\n",
    "    except ValueError as err:\n",
    "        print('No basin geometry available for {}.'.format(wsc_stn))\n",
    "        return None\n",
    "#     closest_radar_stn = stn_df[stn_df['Station Number'] == wsc_stn]['Closest_radar'].values[0]\n",
    "    if len(basin_geom) > 0:\n",
    "        t0 = time.time()\n",
    "        get_img_mask(closest_radar_stn, basin_geom, wsc_stn)\n",
    "        t1 = time.time()\n",
    "        print('Image mask retrieved in {:.1f} s.'.format(t1 - t0))\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image masks for all WSC stations\n",
    "\n",
    "It takes between 1-2 minutes to create the station mask, so loading them once \n",
    "and pre-saving saves considerable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this masking process takes a really long time because \n",
    "# producing the masked array is really slow.\n",
    "# should try finding a better algorithm but 'union' doesn't work.\n",
    "# Maybe there's a workaround?\n",
    "# i = 0\n",
    "# for stn in all_sites[138:]:\n",
    "#     i += 1\n",
    "#     stn_name = stn_df[stn_df['Station Number'] == stn]['Station Name'].values[0]\n",
    "#     print('{} of {}: Starting image masking function on station {}: {}'.format(i, len(all_sites),\n",
    "#                                                                                stn, stn_name))\n",
    "\n",
    "    \n",
    "#     create_mask(stn)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_masking_function(wsc_stn):    \n",
    "    \n",
    "    best_events, test_flow_df, closest_radar_stn = run_AD_from_results(wsc_stn)\n",
    "    if len(best_events) > 0:\n",
    "\n",
    "        event_pairs = get_all_event_pairs(best_events)\n",
    "\n",
    "\n",
    "    #     best_result = get_best_result(wsc_stn)\n",
    "    #     runoff_df = initialize_runoff_dataframe(stn)\n",
    "        stn_da = stn_df[stn_df['Station Number'] == wsc_stn]['DA'].values[0]\n",
    "\n",
    "        gif_images = get_gif_images(wsc_stn, closest_radar_stn, best_events)\n",
    "        time0 = time.time()\n",
    "\n",
    "        mask_folder = 'data/wsc_stn_basin_masks'\n",
    "        mask_path = os.path.join(PROJECT_DIR, mask_folder)\n",
    "        mask_filename = wsc_stn + '.json'\n",
    "        existing_masks = os.listdir(mask_path)\n",
    "        if mask_filename in existing_masks:\n",
    "            try:\n",
    "                mask = pd.read_json(os.path.join(mask_path, mask_filename)).to_numpy()\n",
    "            except ValueError as ve:\n",
    "                print(wsc_stn)\n",
    "                print(ve)\n",
    "\n",
    "            mask_images_and_save(gif_images, mask, wsc_stn, closest_radar_stn)\n",
    "            time1 = time.time()\n",
    "            print('    Time to process {} images for {}: {:.1f}s, ({:.2f}s/image)'.format(len(gif_images), \n",
    "                                                                                      wsc_stn,\n",
    "                                                                                       time1-time0, \n",
    "                                                                                       (time1-time0)/len(gif_images)))\n",
    "        else:\n",
    "            print('    No mask available for {}.'.format(wsc_stn))\n",
    "    else:\n",
    "        print('    No results returned for {}.'.format(wsc_stn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stn_df[stn_df['Station Number'] == '08NE039'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for stn in all_sites[1:]:\n",
    "    print('starting {} of {}: {} ...'.format(i, len(all_sites), stn))\n",
    "    run_masking_function(stn)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubc_polygon = get_ubc_polygon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = get_img_mask(best_radar_stn, basin_geom)\n",
    "# mask = get_img_mask('CASSS', )\n",
    "# mask = get_img_mask('CASAG', ubc_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_images = get_gif_images(test_stn, best_radar_stn)\n",
    "# gif_images = os.listdir(os.path.join(RADAR_IMG_DIR, 'CASAG'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the image file\n",
    "\n",
    "# gif_img = Image.open('data/radar_img/08HF006/200708261000.gif') # 640x480x3 array\n",
    "# label = gif_img.convert('RGB')\n",
    "# img_array = np.asarray(label)\n",
    "# radar_img_array = np.asarray(img_array)[:,:480]\n",
    "# color_bar_array = img_array[144:340, 515:535]\n",
    "# color_bar_img = Image.fromarray(color_bar_array, mode='RGB')\n",
    "# colors = color_bar_img.getcolors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Item | Default Unit | Other Possible Conversions |\n",
    "|---|---|---|\n",
    "| Station Loc. | Lat/Lon | UTM (with zone) | None |\n",
    "| Catchment Boundary | ESRI:102001 | Anything Geopandas can do |\n",
    "| Satellite Radar Img. | None | Radar centre can have UTM, pixels are 1kmx1km | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format WSC station data points into a geodataframe\n",
    "all_stations = stn_df['Station Number'].values\n",
    "\n",
    "geo_df = stn_df[['Station Number', 'Latitude', 'Longitude']]\n",
    "# geo_df.crs = geo_df.'EPSG:4326'\n",
    "wsc_sites_geo = gpd.GeoDataFrame(geo_df, geometry=gpd.points_from_xy(geo_df['Longitude'], geo_df['Latitude']),\n",
    "                                crs='EPSG:4326')\n",
    "# Convert to Mercator for Plotting\n",
    "wsc_sites_geo = wsc_sites_geo.to_crs(\"EPSG:3857\")\n",
    "\n",
    "# Get x and y coordinates\n",
    "wsc_sites_geo['x'] = [geometry.x for geometry in wsc_sites_geo['geometry']]\n",
    "wsc_sites_geo['y'] = [geometry.y for geometry in wsc_sites_geo['geometry']]\n",
    "wsc_df = wsc_sites_geo.drop('geometry', axis = 1).copy()\n",
    "sitesource = ColumnDataSource(wsc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format radar station data points into a geodataframe\n",
    "\n",
    "r_df = pd.DataFrame(radar_stations).T\n",
    "r_df['Latitude'] = [e[0] for e in r_df['lat_lon']]\n",
    "r_df['Longitude'] = [e[1] for e in r_df['lat_lon']]\n",
    "r_df = r_df[['alt_name', 'Latitude', 'Longitude']]\n",
    "r_df_geo = gpd.GeoDataFrame(r_df, geometry=gpd.points_from_xy(r_df['Longitude'], r_df['Latitude']),\n",
    "                                crs='EPSG:4326')\n",
    "# convert to Mercator for plotting\n",
    "# r_sites_geo = r_df_geo.to_crs(\"EPSG:3395\")\n",
    "r_sites_geo = r_df_geo.to_crs(\"EPSG:3857\")\n",
    "\n",
    "# Get x and y coordinates\n",
    "r_sites_geo['x'] = [geometry.x for geometry in r_sites_geo['geometry']]\n",
    "r_sites_geo['y'] = [geometry.y for geometry in r_sites_geo['geometry']]\n",
    "\n",
    "rad_df = r_sites_geo.drop('geometry', axis = 1).copy()\n",
    "radar_sitesource = ColumnDataSource(rad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find bounding box for BC and alberta stations\n",
    "# note Mercator projection is EPSG:3857\n",
    "\n",
    "l_box, r_box = wsc_df['x'].min(), wsc_df['x'].max()\n",
    "t_box, b_box = wsc_df['y'].max(), wsc_df['y'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.tile_providers import Vendors, get_provider\n",
    "import utm\n",
    "\n",
    "# tile_provider = get_provider(Vendors.CARTODBPOSITRON)\n",
    "tile_provider = get_provider(Vendors.STAMEN_TERRAIN_RETINA)\n",
    "\n",
    "# print(bbox)\n",
    "# range bounds supplied in web mercator coordinates\n",
    "p = figure(x_range=(l_box*1.0001, 1.0001*r_box), y_range=(0.98*b_box, 1.01*t_box),\n",
    "           x_axis_type=\"mercator\", y_axis_type=\"mercator\",\n",
    "          width=800, height=400)\n",
    "\n",
    "p.add_tile(tile_provider)\n",
    "\n",
    "p.diamond('x', 'y', source=radar_sitesource, color='black', \n",
    "                 size=8, alpha=0.9, legend_label='Radar Stations')\n",
    "p.circle('x', 'y', source=radar_sitesource, color='blue', \n",
    "                 size=5, alpha=0.2, legend_label='Radar Range',\n",
    "                radius=400000, radius_dimension='max')\n",
    "\n",
    "p.circle('x', 'y', source=sitesource, color='red', \n",
    "                 size=5, alpha=0.3, legend_label='WSC Stations')\n",
    "\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_geodf = gpd.GeoDataFrame(basin_geom,\n",
    "                                crs='EPSG:4326')\n",
    "# convert to Mercator for plotting\n",
    "basin_geo = basin_geodf.to_crs(\"EPSG:3857\")\n",
    "\n",
    "# Get x and y coordinates\n",
    "basin_polygon = list(basin_geo.iloc[0].geometry[0].exterior.coords)\n",
    "\n",
    "basin_df = pd.DataFrame()\n",
    "basin_df['x'] = [e[0] for e in basin_polygon]\n",
    "basin_df['y'] = [e[1] for e in basin_polygon]\n",
    "\n",
    "basin_sitesource = ColumnDataSource(basin_df)\n",
    "\n",
    "l_box, r_box = basin_df['x'].min(), basin_df['x'].max()\n",
    "b_box, t_box = basin_df['y'].min(), basin_df['y'].max()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_stn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(x_range=(l_box*1.0001, 1.0001*r_box), y_range=(0.98*b_box, 1.01*t_box),\n",
    "           x_axis_type=\"mercator\", y_axis_type=\"mercator\",\n",
    "          width=800, height=400)\n",
    "\n",
    "p.add_tile(tile_provider)\n",
    "\n",
    "p.line('x', 'y', source=basin_sitesource, color='blue', \n",
    "     legend_label='WSC 08HB048 Catchment',\n",
    "      line_dash='dashed')\n",
    "\n",
    "p.circle('x', 'y', source=sitesource, color='red', \n",
    "                 size=5, alpha=0.3, legend_label='WSC Stations')\n",
    "\n",
    "p.match_aspect = True\n",
    "# gdf.plot('geometry', ax=ax, color='lightgray', alpha=0.1)\n",
    "# basin_geom.plot(ax=ax, alpha=0.35, edgecolor='k', linewidth=2)\n",
    "# print(stn_df[stn_df['Station Number'] == test_stn])\n",
    "show(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
