{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the best AD input parameters found by the Find_Events.ipynb script,\n",
    "load the results, find the corresponding start and end times for runoff events,\n",
    "query for any missing radar images, and batch mask the images to yield\n",
    "just the pixels representing each station catchment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import sys\n",
    "import math\n",
    "import utm\n",
    "import time\n",
    "\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from geopy import distance\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "import codecs\n",
    "\n",
    "from shapely.geometry import shape, mapping, Polygon\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from bokeh.plotting import ColumnDataSource, output_notebook\n",
    "from bokeh.transform import factor_cmap, factor_mark\n",
    "from bokeh.palettes import Spectral3\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "from radar_scrape import get_radar_img_urls, request_img_files\n",
    "from radar_station_coords import radar_sites\n",
    "from get_station_data import get_daily_runoff\n",
    "output_notebook()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "DB_DIR = os.path.join(BASE_DIR, 'code/hydat_db')\n",
    "PROJECT_DIR = os.path.abspath('')\n",
    "# IMG_DIR = os.path.join(PROJECT_DIR, 'data/radar_img')\n",
    "RADAR_IMG_DIR = os.path.join(PROJECT_DIR, 'data/sorted_radar_images')\n",
    "\n",
    "RESULTS_DIR = os.path.join(PROJECT_DIR, 'data/AD_results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = pd.read_csv(DB_DIR + '/WSC_Stations_Master.csv')\n",
    "stations_df['RADAR_Overlap'] = stations_df['Year To'].astype(int) - 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stations_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_filtered = stations_df[stations_df['RADAR_Overlap'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_filtered = stations_filtered[stations_filtered['Regulation'] == 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_filtered.rename(columns={'Gross Drainage Area (km2)': 'DA'}, inplace=True)\n",
    "stations_filtered = stations_filtered[(stations_filtered['Province'] == 'BC') | (stations_filtered['Province'] == 'AB')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_radar_stn(row):\n",
    "    \"\"\"\n",
    "    To retrieve radar images, we need to find the closest radar location\n",
    "    to the station of interest.  \n",
    "    Input the station number,\n",
    "    returns the location code of the nearest radar station.\n",
    "    \"\"\"\n",
    "    stn_data = row['Station Number']\n",
    "    \n",
    "    s1 = (row['Latitude'], row['Longitude'])\n",
    "    min_dist = 1E6\n",
    "    closest_stn = None\n",
    "    for site in radar_stations.keys():\n",
    "\n",
    "        s2 = [*radar_stations[site]['lat_lon']]        \n",
    "\n",
    "        this_dist = distance.distance(s2, s1).km\n",
    "    \n",
    "        if this_dist < min_dist:\n",
    "            min_dist = this_dist\n",
    "            closest_stn = site\n",
    "        \n",
    "    return closest_stn\n",
    "\n",
    "def calc_distance(row):\n",
    "    wsc_stn_coords = (row['Latitude'], row['Longitude'])\n",
    "    radar = row['Closest_radar']\n",
    "    radar_coords = radar_stations[radar]['lat_lon']\n",
    "    return distance.distance(radar_coords, wsc_stn_coords).km\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def calculate_distance(row):\n",
    "    wsc_stn_coords = [row['Latitude'], row['Longitude']]\n",
    "    radar = row['Closest_radar']\n",
    "   \n",
    "    radar_coords = radar_stations[radar]['lat_lon']\n",
    "\n",
    "    d = distance.distance(radar_coords, wsc_stn_coords).km\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for stations within 200 km of the radar <sigh>\n",
    "stations_filtered['Closest_radar'] = stations_filtered.apply(lambda row: find_closest_radar_stn(row), axis=1)\n",
    "stations_filtered['Dist_to_radar'] = stations_filtered.apply(lambda row: calculate_distance(row), axis=1)\n",
    "# print(stations_filtered['Dist_to_radar'].head())\n",
    "print(len(stations_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_filtered = stations_filtered[stations_filtered['Dist_to_radar'] < 200]\n",
    "print(len(stations_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_df = stations_filtered[np.isfinite(stations_filtered['DA'].astype(float))]\n",
    "stn_df = stn_df[stn_df['DA'].astype(float) >= 10]\n",
    "stn_df = stn_df[stn_df['DA'].astype(float) < 1000].sort_values('DA')\n",
    "# print(stn_df[stn_df['Station Number'] == '08HB048'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stn_df[['Province', 'Station Number', 'Station Name', 'DA', 'Elevation', 'Latitude', 'Longitude', 'RADAR_Overlap']]\n",
    "print('After filtering, there are {} candidate stations.'.format(len(stn_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the results dataframe for the specified site\n",
    "results_folders = os.listdir(RESULTS_DIR)\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "folder_path = os.path.join(RESULTS_DIR, 'sample_55')\n",
    "all_sites = [e.split('_')[0] for e in os.listdir(folder_path)]\n",
    "results_dict = {site: pd.read_csv(os.path.join(folder_path, site + '_results.csv')) for site in all_sites}\n",
    "\n",
    "def get_best_result(site):\n",
    "    ad_df = pd.DataFrame(results_dict[site])\n",
    "    ad_df.drop(labels='Unnamed: 0', inplace=True, axis=1)\n",
    "    ad_df.sort_values('len_results', inplace=True, ascending=False)\n",
    "    return ad_df.iloc[0, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stn = stn_df['Station Number'].values[0]\n",
    "\n",
    "best_result = get_best_result(test_stn)\n",
    "\n",
    "test_stn_info = stn_df[stn_df['Station Number'] == test_stn]\n",
    "\n",
    "test_stn_name = test_stn_info['Station Name'].values[0]\n",
    "test_stn_da = test_stn_info['DA'].values[0]\n",
    "print('Station {} ({}) has a DA of {} km^2'.format(test_stn, test_stn_name, test_stn_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_daily_runoff(test_stn)\n",
    "df['Year'] = df.index.year\n",
    "df['Month'] = df.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by minimum radar date\n",
    "df = df[df.index > pd.to_datetime('2007-05-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Year'] > 2013].plot('Date', 'DAILY_FLOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peak_detection import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks(data, lag=7, threshold=500, influence=0.5):\n",
    "    # Settings (the ones below are examples: choose what is best for your data)\n",
    "#     lag = 5         # lag 5 for the smoothing functions\n",
    "#     threshold = 3.5  # 3.5 standard deviations for signal\n",
    "#     influence = 0.5  # between 0 and 1, where 1 is normal influence, 0.5 is half\n",
    "    # Initialize variables\n",
    "    signals = np.zeros(len(data))            # Initialize signal results\n",
    "    filteredY = np.empty(len(data))\n",
    "    filteredY[:lag] = data[:lag]             # Initialize filtered series\n",
    "    avgFilter = [0]                          # Initialize average filter\n",
    "    stdFilter = [0]                          # Initialize std. filter\n",
    "    avgFilter = {lag: np.mean(data[:lag])}      # Initialize first value\n",
    "    stdFilter = {lag: np.std(data[:lag])}     # Initialize first value\n",
    "    \n",
    "    for i in range(lag + 1, len(data)):\n",
    "        d = data[i]\n",
    "        \n",
    "        af = avgFilter[i-1]\n",
    "        sf = stdFilter[i-1]\n",
    "        \n",
    "        if abs(d - af) > threshold * sf:\n",
    "            if d > af:\n",
    "                signals[i] = 1                     # Positive signal\n",
    "            else:\n",
    "                signals[i] = -1                    # Negative signal\n",
    "\n",
    "            \n",
    "            filteredY[i] = influence*d + (1-influence)*filteredY[i-1]\n",
    "        else:\n",
    "            signals[i] = 0                        # No signal\n",
    "            filteredY[i] = 0\n",
    "        \n",
    "        \n",
    "        # Adjust the filters\n",
    "        avgFilter[i] = np.mean(filteredY[i-lag:i])\n",
    "        stdFilter[i] = np.std(filteredY[i-lag:i])\n",
    "        \n",
    "    return signals, filteredY\n",
    "\n",
    "n_test = 500\n",
    "\n",
    "dats = list(df['DAILY_FLOW'].to_numpy())\n",
    "sigs, f_dat = find_peaks(dats, influence=0.75, lag=7, threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show, output_notebook\n",
    "\n",
    "input_sig = df[['DAILY_FLOW']].copy()\n",
    "signal = np.array(sigs)\n",
    "input_sig['sig'] = signal.copy().astype(int)\n",
    "input_sig['f_sig'] = f_dat\n",
    "\n",
    "foo = input_sig[input_sig['sig'] == 1].copy()\n",
    "p = figure(plot_width=800, plot_height=400, x_axis_type='datetime')\n",
    "# add a circle renderer with a size, color, and alpha\n",
    "p.circle(foo.index, foo['DAILY_FLOW'], size=10, color=\"red\", \n",
    "         alpha=0.5, legend_label='{} pts'.format(len(foo)))\n",
    "# p.line(input_sig.index, input_sig['f_sig'], color='blue')\n",
    "p.line(input_sig.index, input_sig['DAILY_FLOW'], color='blue')\n",
    "# p.line()\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the summer baseflow\n",
    "\n",
    "Break up the May to November records by periods where it comes back to within X% of baseflow.  \n",
    "\n",
    "check durations of these periods, see how many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code from Anomaly detection ML methods article:\n",
    "# https://towardsdatascience.com/machine-learning-for-anomaly-detection-and-condition-monitoring-d4614e7de770\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow\n",
    "\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.core import Dense \n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_df(flow_df, stn_da):\n",
    "\n",
    "    lag_df = flow_df.copy()\n",
    "    \n",
    "    lag_df.rename(columns={'DAILY_FLOW': 'Q'}, inplace=True)\n",
    "\n",
    "    num_lags = int(np.ceil(stn_da / 100) + 5)\n",
    "\n",
    "    for i in range(1,num_lags):\n",
    "        lag_df['Q{}'.format(i)] = lag_df['Q'].shift(i)\n",
    "\n",
    "    lag_df.dropna(inplace=True)\n",
    "    \n",
    "    return lag_df, num_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wsc_station_info_dataframe():\n",
    "    # import master station list\n",
    "    stations_df = pd.read_csv(DB_DIR + '/WSC_Stations_Master.csv')\n",
    "    # filter for stations that have concurrent record with the historical radar record\n",
    "    stations_df['RADAR_Overlap'] = stations_df['Year To'].astype(int) - 2007\n",
    "    stations_filtered = stations_df[stations_df['RADAR_Overlap'] > 0]\n",
    "    # filter for stations that are natural flow regimes\n",
    "    stations_filtered = stations_filtered[stations_filtered['Regulation'] == 'N']\n",
    "    stations_filtered.rename(columns={'Gross Drainage Area (km2)': 'DA'}, inplace=True)\n",
    "    # filter for stations in Alberta and British Columbia\n",
    "    stations_filtered = stations_filtered[(stations_filtered['Province'] == 'BC') | (stations_filtered['Province'] == 'AB')]\n",
    "    \n",
    "    # calculate distance to each radar station\n",
    "    stations_filtered['radar_stn_distance_dict'] = stations_filtered.apply(lambda row: calculate_radar_stn_distances(row), axis=1)    \n",
    "    stations_filtered['closest_radar_station'] = stations_filtered.apply(lambda row: find_closest_radar_stn(row), axis=1)\n",
    "    stations_filtered['radar_distance_km'] = stations_filtered.apply(lambda row: find_closest_radar_stn_distance(row), axis=1)\n",
    "    \n",
    "    # radar range is a 240km radius from the station\n",
    "    stations_filtered = stations_filtered[stations_filtered['radar_distance_km'] < 190]\n",
    "    stn_df = stations_filtered[np.isfinite(stations_filtered['DA'].astype(float))]\n",
    "    # filter for stations greater than 10 km^2 (too small for meaningful results)\n",
    "    stn_df = stn_df[stn_df['DA'].astype(float) >= 10]\n",
    "    # filter for stations smaller than 1000 km^2 (too large and complex)\n",
    "    stn_df = stn_df[stn_df['DA'].astype(float) < 1000].sort_values('DA')\n",
    "    df = stn_df[['Province', 'Station Number', 'Station Name', 'DA', \n",
    "                 'Elevation', 'Latitude', 'Longitude', 'RADAR_Overlap',\n",
    "                'closest_radar_station', 'radar_stn_distance_dict', 'radar_distance_km']]\n",
    "#     print('After filtering, there are {} candidate stations.'.format(len(stn_df)))\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_radar_stn(row):\n",
    "    \"\"\" \n",
    "    Input the dict of all station distances,\n",
    "    Return the location code of the nearest radar station.\n",
    "    \"\"\"\n",
    "    radar_station_distances = row['radar_stn_distance_dict']\n",
    "    min_dist = min(radar_station_distances.items(), key=lambda x: x[1])\n",
    "    return min_dist[0]\n",
    "\n",
    "\n",
    "def find_closest_radar_stn_distance(row):\n",
    "    \"\"\" \n",
    "    Input the dict of all station distances,\n",
    "    Return the location code of the nearest radar station.\n",
    "    \"\"\"\n",
    "    radar_station_distances = row['radar_stn_distance_dict']\n",
    "    min_dist = min(radar_station_distances.items(), key=lambda x: x[1])\n",
    "    return min_dist[1]\n",
    "\n",
    "def calc_distance(wsc_row, radar_station):\n",
    "    wsc_stn_coords = (wsc_row['Latitude'], wsc_row['Longitude'])\n",
    "    radar_coords = radar_stations[radar_station]['lat_lon']\n",
    "    return distance.distance(radar_coords, wsc_stn_coords).km\n",
    "\n",
    "def calculate_all_distances(row, radar_site):\n",
    "    wsc_stn_coords = [row['Latitude'], row['Longitude']]\n",
    "    \n",
    "    radar_coords = radar_stations[radar_site]['lat_lon']\n",
    "    xs = [wsc_stn_coords[1], radar_coords[1]]\n",
    "    ys = [wsc_stn_coords[0], radar_coords[0]]\n",
    "\n",
    "    geom = [Point(xy) for xy in zip(xs, ys)]\n",
    "    gdf = gpd.GeoDataFrame(geometry=geom, crs='epsg:4326')\n",
    "    gdf.to_crs(epsg=3310, inplace=True)\n",
    "    # calculate distance in kilometers\n",
    "    l = gdf.distance(gdf.shift()).values[-1] / 1000\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_radar_stn_distances(row):\n",
    "    distance_dict = {}\n",
    "    for site in radar_stations:\n",
    "        distance_dict[site] = calculate_all_distances(row, site)\n",
    "    return distance_dict\n",
    "\n",
    "def split_train_and_test_data(data, training_months, training_year):\n",
    "    time_range_check = (data.index.year == training_year) & (data.index.month.isin(training_months))\n",
    "    train_data = data[time_range_check]\n",
    "    # the test data is the entire dataset because we want to extract\n",
    "    # extreme events from the training year as well\n",
    "    test_data = data\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):\n",
    "    inv_covariance_matrix = inv_cov_matrix\n",
    "    vars_mean = mean_distr\n",
    "    diff = data - vars_mean\n",
    "    md = []\n",
    "    for i in range(len(diff)):\n",
    "        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))\n",
    "    return md\n",
    "\n",
    "def MD_detectOutliers(dist, extreme=False, verbose=False):\n",
    "    k = 3. if extreme else 2.\n",
    "    threshold = np.mean(dist) * k\n",
    "    outliers = []\n",
    "    for i in range(len(dist)):\n",
    "        if dist[i] >= threshold:\n",
    "            outliers.append(i)  # index of the outlier\n",
    "    return np.array(outliers)\n",
    "\n",
    "def MD_threshold(dist, extreme=False, verbose=False):\n",
    "    k = 3. if extreme else 2.\n",
    "    threshold = np.mean(dist) * k\n",
    "    return threshold\n",
    "\n",
    "def is_pos_def(A):\n",
    "    if np.allclose(A, A.T):\n",
    "        try:\n",
    "            np.linalg.cholesky(A)\n",
    "            return True\n",
    "        except np.linalg.LinAlgError:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_runoff_dataframe(test_stn):\n",
    "    \n",
    "    runoff_df = get_daily_runoff(test_stn)\n",
    "    runoff_df['Q'] = runoff_df['DAILY_FLOW']\n",
    "    runoff_df = runoff_df[['Q']]\n",
    "#     runoff_df['Year'] = runoff_df.index.year\n",
    "#     runoff_df['Month'] = runoff_df.index.month\n",
    "    \n",
    "    # filter by minimum radar date\n",
    "    runoff_df = runoff_df[runoff_df.index > pd.to_datetime('2007-05-31')]\n",
    "    \n",
    "#     runoff_df['Date'] = runoff_df.index.values\n",
    "    \n",
    "    return runoff_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_PCA(X_train, X_test, n_components):\n",
    "    \n",
    "    for n_components_kept in range(2, n_components + 1):\n",
    "\n",
    "        pca = PCA(n_components=n_components_kept, svd_solver= 'full')\n",
    "        X_train_PCA = pca.fit_transform(X_train)\n",
    "        X_train_PCA = pd.DataFrame(X_train_PCA)\n",
    "        X_train_PCA.index = X_train.index\n",
    "\n",
    "        X_test_PCA = pca.transform(X_test)\n",
    "        X_test_PCA = pd.DataFrame(X_test_PCA)\n",
    "        X_test_PCA.index = X_test.index\n",
    "\n",
    "        var_expl = 100*np.sum(pca.explained_variance_ratio_)\n",
    "        if var_expl >= 90:\n",
    "#             print('var > 0.9 in {} components'.format(n_components_kept))\n",
    "            return X_train_PCA, X_test_PCA, var_expl, n_components_kept\n",
    "#     print('var < 0.9 in {} components'.format(n_components_kept))\n",
    "    return X_train_PCA, X_test_PCA, var_expl, n_components_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_params):\n",
    "    \n",
    "    months_str = training_params['train_months'][1:-1]\n",
    "    \n",
    "    if len(months_str) == 4:\n",
    "        training_months = [int(months_str.split(',')[0])]\n",
    "    elif len(months_str) == 2:\n",
    "        training_months = [int(months_str[0])]\n",
    "    else:\n",
    "        training_months = [int(e) for e in months_str.split(', ')]\n",
    "        \n",
    "    training_year = training_params['train_year']\n",
    "#     training_set_len = input_array[2]\n",
    "    wsc_station_num = training_params['wsc_stn']\n",
    "    stn_da = stn_df[stn_df['Station Number'] == wsc_station_num]['DA'].values[0]\n",
    "    training_sample_size = training_params['n_sample']\n",
    "    \n",
    "#     print('Initializing input data...')\n",
    "    \n",
    "    closest_radar_stn = training_params['radar_stn']\n",
    "    \n",
    "    runoff_df = initialize_runoff_dataframe(wsc_station_num)   \n",
    "    \n",
    "    lag_df, num_lags = create_lag_df(runoff_df, stn_da)\n",
    "\n",
    "#     lag_df, closest_radar_stn, runoff_df, num_lags = initialize_input_data(wsc_station_num)\n",
    "#     print('Splitting input flow series into training and test sets...')\n",
    "    dataset_train, dataset_test = split_train_and_test_data(lag_df, training_months, training_year)\n",
    "        \n",
    "    training_set_len = len(dataset_train)\n",
    "    \n",
    "    if len(dataset_train) < 2:\n",
    "        print('exited because dataset_train is too small')\n",
    "#         print(dataset_train)\n",
    "        return pd.DataFrame([]), 0\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(dataset_train), \n",
    "                                  columns=dataset_train.columns, \n",
    "                                  index=dataset_train.index)\n",
    "    # Random shuffle training data\n",
    "    X_train.sample(frac=1)\n",
    "\n",
    "    X_test = pd.DataFrame(scaler.transform(dataset_test), \n",
    "                                 columns=dataset_test.columns, \n",
    "                                 index=dataset_test.index)\n",
    "    \n",
    "#     print('Perform PCA on the lagged runoff series matrix...')\n",
    "    X_train_PCA, X_test_PCA, var_expl, n_components = do_PCA(X_train, X_test, num_lags)\n",
    "    \n",
    "    data_train = np.array(X_train_PCA.values)\n",
    "    data_test = np.array(X_test_PCA.values)\n",
    "    \n",
    "    def cov_matrix(data, verbose=False):\n",
    "        covariance_matrix = np.cov(data, rowvar=False)\n",
    "        if is_pos_def(covariance_matrix):\n",
    "            inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n",
    "            if is_pos_def(inv_covariance_matrix):\n",
    "                return True, covariance_matrix, inv_covariance_matrix\n",
    "            else:\n",
    "                print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n",
    "                return False, None, None\n",
    "        else:\n",
    "            print(\"Error: Covariance Matrix is not positive definite!\")\n",
    "            return False, None, None\n",
    "\n",
    "               \n",
    "    cov_test, cov_matrix, inv_cov_matrix = cov_matrix(data_train)\n",
    "    \n",
    "    if cov_test == False:\n",
    "        return pd.DataFrame([]), 0\n",
    "\n",
    "    mean_distr = data_train.mean(axis=0)\n",
    "\n",
    "    dist_test = MahalanobisDist(inv_cov_matrix, mean_distr, data_test, verbose=False)\n",
    "    dist_train = MahalanobisDist(inv_cov_matrix, mean_distr, data_train, verbose=False)\n",
    "    threshold = MD_threshold(dist_train, extreme = True)\n",
    "    \n",
    "    anomaly_train = pd.DataFrame()\n",
    "    anomaly_train['Mob dist']= dist_train\n",
    "    anomaly_train['Thresh'] = threshold\n",
    "    # If Mob dist above threshold: Flag as anomaly\n",
    "    anomaly_train['Anomaly'] = anomaly_train['Mob dist'] > anomaly_train['Thresh']\n",
    "    anomaly_train.index = X_train_PCA.index\n",
    "    anomaly = pd.DataFrame()\n",
    "    anomaly['Mob dist']= dist_test\n",
    "    anomaly['Thresh'] = threshold\n",
    "    anomaly['num_components_kept'] = n_components\n",
    "    # If Mob dist above threshold: Flag as anomaly\n",
    "    anomaly['Anomaly'] = anomaly['Mob dist'] > anomaly['Thresh']\n",
    "    anomaly.index = X_test_PCA.index\n",
    "    anomaly.head()\n",
    "    \n",
    "    anomaly_alldata = pd.concat([anomaly_train, anomaly], sort=True)\n",
    "    \n",
    "    event_times = np.where(anomaly_alldata['Anomaly'].values[:-1] != anomaly_alldata['Anomaly'].values[1:])[0]\n",
    "    events = pd.merge(lag_df, anomaly_alldata.iloc[event_times,:], how='inner', \n",
    "                      left_index=True, right_index=True)\n",
    "\n",
    "    events = events.loc[~events.index.duplicated(keep='first')]\n",
    "    \n",
    "    if len(events) == 0:\n",
    "        print('exited because len(events) == 0')\n",
    "        return pd.DataFrame([]), 0\n",
    "    elif events.iloc[0]['Anomaly'] == True:\n",
    "        events = events.iloc[1:]\n",
    "        \n",
    "    # create a column of time difference between events in days\n",
    "    events['dt_days'] = events.index.to_series().diff(1)    \n",
    "\n",
    "    a = time.time()\n",
    "\n",
    "    last_event_end = False\n",
    "\n",
    "    new_events = pd.DataFrame()\n",
    "\n",
    "    # iterate through the detected event pairs \n",
    "    for i in np.arange(0, len(events) - 1, 2):\n",
    "        # parse a single event pair\n",
    "        this_event = events.iloc[i:i+2]\n",
    "        \n",
    "        check_sign_switch = this_event['Anomaly'].values[0] != this_event['Anomaly'].values[1]\n",
    "        concurrent_wsc = lag_df[(lag_df.index >= this_event.index.values[0]) & (lag_df.index <= this_event.index.values[1])][['Q']]\n",
    "        peak_in_middle = check_peak_in_middle(this_event, concurrent_wsc)\n",
    "\n",
    "        if (check_sign_switch) & (peak_in_middle):\n",
    "\n",
    "            # get the start date\n",
    "            this_event_start = pd.to_datetime(this_event[this_event['Anomaly'] == False].index.values[0])\n",
    "            # get the end date\n",
    "            this_event_end = pd.to_datetime(this_event[this_event['Anomaly'] == True].index.values[0])\n",
    "\n",
    "            new_event_start = lag_df[lag_df.index == this_event_start][['Q']]\n",
    "            new_event_end = lag_df[lag_df.index == this_event_end][['Q']]\n",
    "\n",
    "            adjusted_start_date = pd.to_datetime(adjust_edge_date(this_event_start, lag_df[['Q']], 'start'))\n",
    "\n",
    "            new_event_start = lag_df[lag_df.index == adjusted_start_date][['Q']]\n",
    "\n",
    "            if last_event_end is not False:\n",
    "\n",
    "                # find if the start date is on the rising limb - adjust if so\n",
    "                if adjusted_start_date < last_event_end:\n",
    "                    new_event_start = lag_df[lag_df.index == this_event_start][['Q']]\n",
    "\n",
    "            new_event_start['timing'] = 'start'\n",
    "            new_event_end['timing'] = 'end'\n",
    "\n",
    "            min_time_check = (new_event_end.index - new_event_start.index).days > 1\n",
    "            max_time_check = (new_event_end.index - new_event_start.index).days <= 14\n",
    "            start_month = new_event_start.index.month\n",
    "            end_month = new_event_end.index.month\n",
    "            season_check = (start_month > 5) & (start_month < 11) & (end_month <= 11)\n",
    "\n",
    "            if (min_time_check) & (max_time_check) & (season_check):\n",
    "                new_events = new_events.append(new_event_start)\n",
    "                new_events = new_events.append(new_event_end)\n",
    "\n",
    "            last_event_end = pd.to_datetime(this_event_end)\n",
    "\n",
    "\n",
    "    b = time.time()\n",
    "#     print(b - a)\n",
    "\n",
    "    new_events.sort_index(inplace=True)\n",
    "    \n",
    "\n",
    "    new_events['dt_days'] = new_events.index.to_series().diff(1)\n",
    "    new_events['wsc_station'] = wsc_station_num\n",
    "    new_events['training_year'] = training_year\n",
    "    new_events['training_months'] = [training_months for n in range(len(new_events))]\n",
    "    new_events['training_set_len'] = training_set_len\n",
    "    new_events['m_threshold'] = threshold\n",
    "    new_events['var_explained'] = var_expl\n",
    "    new_events['n_components'] = n_components\n",
    "    new_events['num_lags'] = num_lags\n",
    "                \n",
    "    return new_events, n_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_edge_date(initial_date, data, direction):\n",
    "    \"\"\"\n",
    "    If the start flow is on a rising limb, adjust the start to the start of the runoff event.\n",
    "    \"\"\"\n",
    "    initial_val = data[data.index == initial_date]['Q']\n",
    "\n",
    "    if direction == 'end':\n",
    "        search_criteria = (data.index <= initial_date + pd.Timedelta('7 days')) & (data.index >= initial_date)\n",
    "        search_direction = -1\n",
    "    elif direction == 'start':\n",
    "        search_criteria = (data.index >= initial_date - pd.Timedelta('7 days')) & (data.index <= initial_date)\n",
    "        search_direction = 1\n",
    "        \n",
    "        \n",
    "    extended_week_vals = data[search_criteria][['Q']]\n",
    "    extended_week_vals['diff'] = extended_week_vals.diff(periods=search_direction)\n",
    "    extended_week_vals['pct_change'] = 100 * extended_week_vals['diff'] / extended_week_vals['Q']\n",
    "\n",
    "    if direction == 'start':\n",
    "        try:\n",
    "            extended_week_vals.at[extended_week_vals.index.min(),'diff'] = -1\n",
    "            change_point_row = extended_week_vals[['pct_change']].idxmax()\n",
    "            if len(change_point_row) > 1:\n",
    "                change_point_date = extended_week_vals.loc[change_point_row - pd.DateOffset(1)].index.values[0]\n",
    "                adjusted_date = change_point_date\n",
    "            else:\n",
    "                adjusted_date = initial_date\n",
    "            \n",
    "        except ValueError as err:\n",
    "#                 print(err)\n",
    "#                 print('no change')\n",
    "            adjusted_date = initial_date\n",
    "\n",
    "    elif direction == 'end':\n",
    "#             adjusted_dates.append(s)\n",
    "        try:\n",
    "            change_point = extended_week_vals[extended_week_vals['diff'] < 0][['Q']].idxmin().values[0]\n",
    "            adjusted_dates = change_point\n",
    "#                 print(s, change_point)\n",
    "\n",
    "        except ValueError as err:\n",
    "#                 print(extended_week_vals['Q'].min())\n",
    "            change_point = extended_week_vals[extended_week_vals['Q'] == extended_week_vals['Q'].min()].index.values[0]\n",
    "#                 print(err)\n",
    "#                 print('no change')\n",
    "            adjusted_date = change_point\n",
    "\n",
    "            \n",
    "    return pd.to_datetime(adjusted_date)\n",
    "\n",
    "def check_peak_in_middle(event, data):\n",
    "    \"\"\"\n",
    "    Ensure there is a peak between the start and end points\n",
    "    so we aren't targeting a non-runoff event.\n",
    "    \"\"\"\n",
    "    start_time = event.index.values[0] \n",
    "    end_time = event.index.values[-1]\n",
    "    max_time = data[data['Q'] == data['Q'].max()].index.values[0]\n",
    "    if (max_time == start_time) | (max_time == end_time):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to test a single station and view AD results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_AD_from_results(test_stn):\n",
    "\n",
    "    test_flow_df = get_daily_runoff(test_stn)\n",
    "    test_flow_df = test_flow_df[test_flow_df.index.year >= 2007]\n",
    "    test_flow_df.rename(columns={'DAILY_FLOW': 'Q'}, inplace=True)\n",
    "    test_flow_df = test_flow_df[['Q']]\n",
    "\n",
    "    closest_radar_stn = stn_df[stn_df['Station Number'] == test_stn]['Closest_radar'].values[0]\n",
    "\n",
    "    best_training_params = get_best_result(test_stn)\n",
    "\n",
    "    n_sample = best_training_params['n_sample']\n",
    "    \n",
    "    best_events, msg = train_model(best_training_params)\n",
    "\n",
    "    return best_events, test_flow_df, closest_radar_stn\n",
    "\n",
    "\n",
    "# print(best_training_params)\n",
    "best_events, test_flow_df, closest_radar_stn = run_AD_from_results(test_stn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=800, plot_height=400, x_axis_type='datetime')\n",
    "\n",
    "e1 = best_events[best_events['timing'] == 'start']\n",
    "e2 = best_events[best_events['timing'] == 'end']\n",
    "\n",
    "p.circle(e1.index, e1['Q'], color='red', alpha=0.5, size=10, legend_label='start')\n",
    "p.circle(e2.index, e2['Q'], color='blue', alpha=0.5, size=10, legend_label='end')\n",
    "\n",
    "# p.line(input_sig.index, input_sig['f_sig'], color='blue')\n",
    "p.line(test_flow_df.index, test_flow_df['Q'], color='blue')\n",
    "# p.line()\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid plot of individual events\n",
    "\n",
    "plots = []\n",
    "\n",
    "for i in np.arange(0, len(best_events) - 1, 2):\n",
    "    \n",
    "    # parse a single event pair\n",
    "    this_event = best_events.iloc[i:i+2]\n",
    "    \n",
    "    s1 = figure(background_fill_color=\"#fafafa\", x_axis_type='datetime')\n",
    "    \n",
    "    s1.circle(this_event.index, this_event['Q'], \n",
    "              size=12, alpha=0.8, color=\"red\")#, legend_label='{estimated endpoints}')\n",
    "    s1.xaxis.major_label_orientation = math.pi/2\n",
    "    this_start = pd.to_datetime(this_event.index.values[0])\n",
    "    this_end = pd.to_datetime(this_event.index.values[1])\n",
    "    this_dat = test_flow_df[(test_flow_df.index >= this_start) & (test_flow_df.index <= this_end)][['Q']]\n",
    "    \n",
    "    if (this_end.month < 12) & (this_start.month > 5):\n",
    "        year = this_event.index.year.values[0]\n",
    "        month = this_event.index.month.values[0]\n",
    "        day = this_event.index.day.values[0]\n",
    "        date = '{}-{}-{}'.format(year, month, day)\n",
    "        s1.line(this_dat.index, this_dat['Q'], color='blue')\n",
    "        plots.append(s1)\n",
    "\n",
    "print('there are {} plots'.format(len(plots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(plots) < 6:\n",
    "#     grid = gridplot(plots, plot_width=150, plot_height=150)\n",
    "# else:\n",
    "n_cols = 8\n",
    "n_rows = int(np.ceil(len(plots) / n_cols))\n",
    "\n",
    "\n",
    "g = []\n",
    "for i in range(0, len(plots), n_cols):\n",
    "    g += [plots[i:i+n_cols]]\n",
    "grid = gridplot(g, plot_width=150, plot_height=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to batch process all station events and aquire corresponding radar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_event_pairs(best_events):\n",
    "    event_pairs = []\n",
    "    for i in np.arange(0, len(best_events) - 1, 2):\n",
    "        # parse a single event pair\n",
    "        this_event = best_events.iloc[i:i+2]\n",
    "        date_pair = [e.astype(str).replace('T', ' ').split('.')[0].split(' ')[0] for e in this_event.index.values]\n",
    "        this_start = pd.to_datetime(this_event.index.values[0])\n",
    "        this_month = this_start.month\n",
    "        if (this_month > 5) & (this_month <= 11) & (this_start > pd.to_datetime('2007-01-01')):\n",
    "            event_pairs.append(date_pair)\n",
    "    return event_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for stn in ['08GA072']:#all_sites: \n",
    "    i += 1\n",
    "    best_events, test_flow_df, closest_radar_stn = run_AD_from_results(stn)\n",
    "    event_pairs = get_all_event_pairs(best_events)\n",
    "    all_img_urls = get_radar_img_urls(event_pairs, closest_radar_stn)\n",
    "    n_images = len(all_img_urls)\n",
    "    print('{} of {}: {} image files to query for {} with {} events identified.'.format(i, len(all_sites), \n",
    "                                                                                       len(all_img_urls), stn, \n",
    "                                                                                       len(event_pairs)))\n",
    "    time0 = time.time()    \n",
    "    request_img_files(all_img_urls, stn, closest_radar_stn)\n",
    "    time1 = time.time()    \n",
    "    print('    {:.0f}s to query all images'.format(time1 - time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Radar, Mask with Catchment, and Clip all Retrieved Images\n",
    "\n",
    "First, make sure basin geometry is available before requesting all images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "\n",
    "\n",
    "## Further Work\n",
    "\n",
    "Find all the stations that have radar overlap.  \n",
    "\n",
    "Find a bunch of concurrent runoff events, and estimate the error based on the overlapping radar info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ubc_polygon():\n",
    "    points = [-123.1876189229259,49.27349631820059, -123.2168376964195,49.27886813665509,\n",
    "              -123.250951627253,49.28123248845857, -123.2686880871375,49.26628206996413, \n",
    "              -123.2577985144227,49.25107796072373, -123.2296736741512,49.23345938143058, \n",
    "              -123.2086850019333,49.22474925352834, -123.1911799345363,49.22020024319045, \n",
    "              -123.1852881427934,49.2196036807795, -123.1876189229259,49.27349631820059, ]\n",
    "    lat_list = []\n",
    "    lon_list = []\n",
    "    for i in range(0, len(points), 2):\n",
    "        lat_list.append(points[i])\n",
    "        lon_list.append(points[i + 1])\n",
    "#     gdb_path = os.path.join(PROJECT_DIR, path)\n",
    "#     data = gpd.read_file(gdb_path, driver='kmz')\n",
    "#     return data.geometry\n",
    "    minx = min(lat_list)\n",
    "    maxx = max(lat_list)\n",
    "    miny = min(lon_list)\n",
    "    maxy = max(lon_list)\n",
    "    bbox = pd.DataFrame({'minx': [minx], 'miny': [miny],\n",
    "                        'maxx': [maxx], 'maxy': [maxy]})\n",
    "    return Polygon(zip(lat_list, lon_list)), bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.speedups\n",
    "shapely.speedups.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_images_and_save(gif_images, mask, test_stn, best_radar_stn):\n",
    "    print(best_radar_stn)\n",
    "    origin_path = os.path.join(RADAR_IMG_DIR, best_radar_stn)\n",
    "    save_path = os.path.join(PROJECT_DIR, 'data/masked_img/{}'.format(test_stn))\n",
    "    for t_img in gif_images:\n",
    "        new_filename = t_img.split('.')[0] + '_crp.gif'\n",
    "        time0 = time.time()      \n",
    "        origin_full_path = os.path.join(origin_path, t_img)\n",
    "        new_file_path = os.path.join(save_path, new_filename)\n",
    "        # there are a large number of empty images that get returned\n",
    "        # from image retrievel.  DON'T DELETE THEM or you will \n",
    "        # request them many times over in the AD function.\n",
    "        # Instead, filter them out here.\n",
    "        if os.path.getsize(origin_full_path) > 100:\n",
    "            gif_img = Image.open(origin_full_path).convert('RGB')\n",
    "            img_array = np.asarray(gif_img)[:,:480].astype(np.uint8)        \n",
    "            filtered_img = mask_image(img_array, mask)\n",
    "            time1 = time.time()\n",
    "            try:\n",
    "                cropped_array = bbox2(filtered_img)\n",
    "                cropped_img = Image.fromarray(cropped_array.astype(np.uint8))\n",
    "                cropped_img.save(os.path.join(save_path, new_filename))\n",
    "            except Exception as e:\n",
    "                print('Something broke in the image mask process for {}'.format(test_stn))\n",
    "                print(e)\n",
    "                break\n",
    "\n",
    "\n",
    "def get_event_times(df):\n",
    "    event_pairs = []\n",
    "    for i in np.arange(0, len(df) - 1, 2):    \n",
    "        # parse a single event pair\n",
    "        this_event = df.iloc[i:i+2]\n",
    "        date_pair = [e.astype(str).replace('T', ' ').split('.')[0].split(' ')[0] for e in this_event.index.values]\n",
    "        this_start = pd.to_datetime(this_event.index.values[0])\n",
    "        this_month = this_start.month\n",
    "        if (this_month > 5) & (this_month <= 11) & (this_start > pd.to_datetime('2007-01-01')):\n",
    "            event_pairs.append(date_pair)\n",
    "    return event_pairs\n",
    "    \n",
    "    \n",
    "def get_possible_filenames_from_event_time(event_pair):\n",
    "    filenames = []\n",
    "    for p in event_pair:\n",
    "        start, end = p[0], p[1]\n",
    "        datetime_range = [str(pd.to_datetime(e).strftime('%Y%m%d%H%M')) + '.gif' for e in pd.date_range(pd.to_datetime(start), \n",
    "                                       pd.to_datetime(end),\n",
    "                                      freq='1H').values]\n",
    "        filenames += datetime_range\n",
    "    return set(filenames)\n",
    "\n",
    "\n",
    "def get_radar_images_for_events(wsc_stn, radar_stn, best_events):\n",
    "    event_df = best_events\n",
    "    event_pairs = get_event_times(event_df)\n",
    "    \n",
    "    files_to_search = get_possible_filenames_from_event_time(event_pairs)\n",
    "    \n",
    "    existing_files = os.listdir(os.path.join(RADAR_IMG_DIR, radar_stn))\n",
    "    \n",
    "#     new_files = np.setdiff1d(files_to_search, existing_files)\n",
    "    new_files = files_to_search.intersection(existing_files)\n",
    "    return new_files\n",
    "\n",
    "\n",
    "def get_gif_images(test_stn, closest_radar_stn, best_events):\n",
    "    gif_path = 'data/sorted_radar_images/{}'.format(closest_radar_stn)\n",
    "    save_path = 'data/masked_img/{}'.format(test_stn)\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    return list(get_radar_images_for_events(test_stn, closest_radar_stn, best_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pixel_coordinates(closest_stn):\n",
    "    # note that the pixel coordinates are projected to NAD83\n",
    "    # coordinate system and when I figure out \n",
    "    # how to re-project without 20km error I will\n",
    "    # try to estimate the error accordingly.\n",
    "    img_coord_path = 'data/radar_img_pixel_coords'\n",
    "    img_coord_file = [f for f in os.listdir(img_coord_path) if closest_stn in f][0]\n",
    "    geo_df = gpd.read_file(os.path.join(img_coord_path, img_coord_file))\n",
    "#     print('what is this?')\n",
    "#     print(geo_df.crs)\n",
    "#     geo_df = geo_df.to_crs('EPSG:3153')\n",
    "    \n",
    "    return geo_df\n",
    "\n",
    "\n",
    "def get_img_mask(closest_stn, basin_geom, wsc_stn):\n",
    "    # take in the basin geometry and its corresponding radar station\n",
    "    # return a 480x480 boolean matrix where True represents\n",
    "    # pixels that fall within the basin polygon\n",
    "    # note that the points are stored in (lat, lon) tuples\n",
    "    # which corresponds to y, x\n",
    "#     print('pulling geometries together...')\n",
    "    basin_geom = basin_geom.to_crs('epsg:4326')\n",
    "    print('Basin polygon crs: ', basin_geom.crs)\n",
    "    print('bason bounds')\n",
    "    print(basin_geom.bounds)\n",
    "#     print('basin geom bounds:')\n",
    "#     print(basin_geom.bounds)\n",
    "    basin_geom_data = basin_geom.geometry\n",
    "#     print(basin_geom_data)\n",
    "    \n",
    "    mask_folder = 'data/wsc_stn_basin_masks'\n",
    "    mask_path = os.path.join(PROJECT_DIR, mask_folder)\n",
    "    mask_filename = wsc_stn + '.json'\n",
    "    existing_masks = os.listdir(mask_path)\n",
    "\n",
    "#     if mask_filename in existing_masks:\n",
    "#         return pd.read_json(os.path.join(mask_path, mask_filename)).to_numpy()\n",
    "#     else:\n",
    "#         print('No image mask exists.  Try creating one...')\n",
    "    geo_df = get_pixel_coordinates(closest_stn)#.to_numpy().flatten()\n",
    "\n",
    "    # match the crs of the basin polygon\n",
    "#     geo_df = geo_df.to_crs('EPSG:4326')\n",
    "#     geo_df = geo_df.to_crs('EPSG:3153')\n",
    "    \n",
    "    print('Image crs: ', geo_df.crs)\n",
    "    print('image geo_df bounds')\n",
    "    print(geo_df.total_bounds)\n",
    "#     print('...')\n",
    "    \n",
    "    pip_mask = geo_df.within(basin_geom.loc[0, 'geometry'])\n",
    "    \n",
    "    reshaped_mask = np.array(pip_mask).reshape(480, 480)\n",
    "\n",
    "    img_mask_df = pd.DataFrame(reshaped_mask)\n",
    "\n",
    "    img_mask_df.to_json(os.path.join(mask_path, mask_filename), orient='columns')\n",
    "    \n",
    "    print('saved json')\n",
    "\n",
    "\n",
    "def bbox2(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    ymin, ymax = np.where(rows)[0][[0, -1]]\n",
    "    xmin, xmax = np.where(cols)[0][[0, -1]]\n",
    "    return img[ymin:ymax+1, xmin:xmax+1]\n",
    "\n",
    "\n",
    "@jit\n",
    "def mask_image(img_array, mask):\n",
    "    mask_idx = np.where(mask==0)\n",
    "    rows = mask_idx[0]\n",
    "    cols = mask_idx[1]\n",
    "    filtered_img = np.zeros((480, 480, 3))\n",
    "    for n in range(480):\n",
    "        for m in range(480):\n",
    "            if mask[n, m] == 0:\n",
    "                filtered_img[n, m] = 0\n",
    "            else:\n",
    "                filtered_img[n, m] = img_array[n, m]\n",
    "    return filtered_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(wsc_stn):\n",
    "    \n",
    "    best_events, test_flow_df, closest_radar_stn = run_AD_from_results(wsc_stn)\n",
    "    event_pairs = get_all_event_pairs(best_events)\n",
    "    \n",
    "    print('closest radar = ', closest_radar_stn)\n",
    "    \n",
    "    stn_da = stn_df[stn_df['Station Number'] == wsc_stn]['DA'].values[0]\n",
    "    \n",
    "    try:\n",
    "        basin_geom = get_basin_geometry(wsc_stn)\n",
    "    except ValueError as err:\n",
    "        print('No basin geometry available for {}.'.format(wsc_stn))\n",
    "        return None\n",
    "#     closest_radar_stn = stn_df[stn_df['Station Number'] == wsc_stn]['Closest_radar'].values[0]\n",
    "    print('Retrieve image mask...')\n",
    "    t0 = time.time()\n",
    "    mask = get_img_mask(closest_radar_stn, basin_geom, wsc_stn)\n",
    "    t1 = time.time()\n",
    "    print('Image mask retrieved in {:.1f} s.'.format(t1 - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polygon(stn):\n",
    "    gdb_path = os.path.join(DB_DIR, 'WSC_Basins.gdb.zip')\n",
    "    data = gpd.read_file(gdb_path, driver='FileGDB', layer='EC_{}_1'.format(stn))\n",
    "    return data\n",
    "\n",
    "def get_basin_geometry(test_stn):\n",
    "    basin_geom = get_polygon(test_stn)\n",
    "    # reproject to EPSG: 3395 (mercator) for plotting\n",
    "    # or to coincide with radar image coordinates use\n",
    "    # original WSC basin polygon is EPSG: 4269 (NAD83)\n",
    "    # WGS 84 is EPSG: 4326\n",
    "    basin_bbox = basin_geom.bounds\n",
    "    return basin_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_masking_function(wsc_stn):\n",
    "    \n",
    "    \n",
    "    best_events, test_flow_df, closest_radar_stn = run_AD_from_results(wsc_stn)\n",
    "    if len(best_events) > 0:\n",
    "\n",
    "        event_pairs = get_all_event_pairs(best_events)\n",
    "\n",
    "\n",
    "    #     best_result = get_best_result(wsc_stn)\n",
    "    #     runoff_df = initialize_runoff_dataframe(stn)\n",
    "        stn_da = stn_df[stn_df['Station Number'] == wsc_stn]['DA'].values[0]\n",
    "\n",
    "        gif_images = get_gif_images(wsc_stn, closest_radar_stn, best_events)\n",
    "        time0 = time.time()\n",
    "\n",
    "        mask_folder = 'data/wsc_stn_basin_masks'\n",
    "        mask_path = os.path.join(PROJECT_DIR, mask_folder)\n",
    "        mask_filename = wsc_stn + '.json'\n",
    "        existing_masks = os.listdir(mask_path)\n",
    "        if mask_filename in existing_masks:\n",
    "            try:\n",
    "                mask = pd.read_json(os.path.join(mask_path, mask_filename)).to_numpy()\n",
    "            except ValueError as ve:\n",
    "                print(wsc_stn)\n",
    "                print(ve)\n",
    "\n",
    "            mask_images_and_save(gif_images, mask, wsc_stn, closest_radar_stn)\n",
    "            time1 = time.time()\n",
    "            print('Time to process {} images for {}: {:.1f}s, ({:.2f}s/image)'.format(len(gif_images), \n",
    "                                                                                      wsc_stn,\n",
    "                                                                                       time1-time0, \n",
    "                                                                                       (time1-time0)/len(gif_images)))\n",
    "        else:\n",
    "            print('No mask available for {}.'.format(wsc_stn))\n",
    "    else:\n",
    "        print('No results returned for {}.'.format(wsc_stn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for stn in ['08NE039']:#all_sites[:10] + ['08NE039']:\n",
    "    i += 1\n",
    "    stn_name = stn_df[stn_df['Station Number'] == stn]['Station Name'].values[0]\n",
    "    print('({} / {}): Starting image masking function on station {}: {}'.format(i, len(all_sites),\n",
    "                                                                               stn, stn_name))\n",
    "\n",
    "    create_mask(stn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_stn = '08NE039'\n",
    "for stn in ['08NE039']:#all_sites[7:8]:\n",
    "    run_masking_function(stn)\n",
    "    \n",
    "# masked_img_path = os.path.join(PROJECT_DIR, 'data/masked_img/{}'.format(test_stn))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "# for m_img in os.listdir(masked_img_path)[:1]:\n",
    "#     image = Image.open(os.path.join(masked_img_path, m_img))\n",
    "\n",
    "# plt.imshow(image)\n",
    "# ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image masks for all WSC stations\n",
    "\n",
    "It takes between 1-2 minutes to create the station mask, so loading them once \n",
    "and pre-saving saves considerable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stn_df[['Station Number', 'DA']].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this masking process takes a really long time because \n",
    "# producing the masked array is really slow.\n",
    "# should try finding a better algorithm but 'union' doesn't work.\n",
    "# Maybe there's a workaround?\n",
    "i = 0\n",
    "for stn in ['08NE039']:#all_sites:\n",
    "    i += 1\n",
    "    stn_name = stn_df[stn_df['Station Number'] == stn]['Station Name'].values[0]\n",
    "    print('{} of {}: Starting image masking function on station {}: {}'.format(i, len(all_sites),\n",
    "                                                                               stn, stn_name))\n",
    "\n",
    "    \n",
    "    create_mask(stn)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubc_polygon = get_ubc_polygon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = get_img_mask(best_radar_stn, basin_geom)\n",
    "# mask = get_img_mask('CASSS', )\n",
    "# mask = get_img_mask('CASAG', ubc_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_images = get_gif_images(test_stn, best_radar_stn)\n",
    "# gif_images = os.listdir(os.path.join(RADAR_IMG_DIR, 'CASAG'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "mask_images_and_save(gif_images, mask, test_stn, best_radar_stn)\n",
    "# mask_images_and_save(gif_images, mask, 'UBC', 'CASAG')\n",
    "time1 = time.time()\n",
    "\n",
    "print('Time to process {} images: {:.1f}s, ({:.2f}s/image)'.format(len(gif_images), \n",
    "                                                                   time1-time0, \n",
    "                                                                   (time1-time0)/len(gif_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the image file\n",
    "\n",
    "# gif_img = Image.open('data/radar_img/08HF006/200708261000.gif') # 640x480x3 array\n",
    "# label = gif_img.convert('RGB')\n",
    "# img_array = np.asarray(label)\n",
    "# radar_img_array = np.asarray(img_array)[:,:480]\n",
    "# color_bar_array = img_array[144:340, 515:535]\n",
    "# color_bar_img = Image.fromarray(color_bar_array, mode='RGB')\n",
    "# colors = color_bar_img.getcolors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_lat = test_stn_info['Latitude'].values[0]\n",
    "stn_lon = test_stn_info['Longitude'].values[0]\n",
    "\n",
    "stn_coords = (stn_lon, stn_lat)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Item | Default Unit | Other Possible Conversions |\n",
    "|---|---|---|\n",
    "| Station Loc. | Lat/Lon | UTM (with zone) | None |\n",
    "| Catchment Boundary | ESRI:102001 | Anything Geopandas can do |\n",
    "| Satellite Radar Img. | None | Radar centre can have UTM, pixels are 1kmx1km | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format WSC station data points into a geodataframe\n",
    "all_stations = stn_df['Station Number'].values\n",
    "\n",
    "geo_df = stn_df[['Station Number', 'Latitude', 'Longitude']]\n",
    "# geo_df.crs = geo_df.'EPSG:4326'\n",
    "wsc_sites_geo = gpd.GeoDataFrame(geo_df, geometry=gpd.points_from_xy(geo_df['Longitude'], geo_df['Latitude']),\n",
    "                                crs='EPSG:4326')\n",
    "# Convert to Mercator for Plotting\n",
    "wsc_sites_geo = wsc_sites_geo.to_crs(\"EPSG:3857\")\n",
    "\n",
    "# Get x and y coordinates\n",
    "wsc_sites_geo['x'] = [geometry.x for geometry in wsc_sites_geo['geometry']]\n",
    "wsc_sites_geo['y'] = [geometry.y for geometry in wsc_sites_geo['geometry']]\n",
    "wsc_df = wsc_sites_geo.drop('geometry', axis = 1).copy()\n",
    "sitesource = ColumnDataSource(wsc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format radar station data points into a geodataframe\n",
    "\n",
    "r_df = pd.DataFrame(radar_stations).T\n",
    "r_df['Latitude'] = [e[0] for e in r_df['lat_lon']]\n",
    "r_df['Longitude'] = [e[1] for e in r_df['lat_lon']]\n",
    "r_df = r_df[['alt_name', 'Latitude', 'Longitude']]\n",
    "r_df_geo = gpd.GeoDataFrame(r_df, geometry=gpd.points_from_xy(r_df['Longitude'], r_df['Latitude']),\n",
    "                                crs='EPSG:4326')\n",
    "# convert to Mercator for plotting\n",
    "# r_sites_geo = r_df_geo.to_crs(\"EPSG:3395\")\n",
    "r_sites_geo = r_df_geo.to_crs(\"EPSG:3857\")\n",
    "\n",
    "# Get x and y coordinates\n",
    "r_sites_geo['x'] = [geometry.x for geometry in r_sites_geo['geometry']]\n",
    "r_sites_geo['y'] = [geometry.y for geometry in r_sites_geo['geometry']]\n",
    "\n",
    "rad_df = r_sites_geo.drop('geometry', axis = 1).copy()\n",
    "radar_sitesource = ColumnDataSource(rad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find bounding box for BC and alberta stations\n",
    "# note Mercator projection is EPSG:3857\n",
    "\n",
    "l_box, r_box = wsc_df['x'].min(), wsc_df['x'].max()\n",
    "t_box, b_box = wsc_df['y'].max(), wsc_df['y'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.tile_providers import Vendors, get_provider\n",
    "import utm\n",
    "\n",
    "# tile_provider = get_provider(Vendors.CARTODBPOSITRON)\n",
    "tile_provider = get_provider(Vendors.STAMEN_TERRAIN_RETINA)\n",
    "\n",
    "# print(bbox)\n",
    "# range bounds supplied in web mercator coordinates\n",
    "p = figure(x_range=(l_box*1.0001, 1.0001*r_box), y_range=(0.98*b_box, 1.01*t_box),\n",
    "           x_axis_type=\"mercator\", y_axis_type=\"mercator\",\n",
    "          width=800, height=400)\n",
    "\n",
    "p.add_tile(tile_provider)\n",
    "\n",
    "p.diamond('x', 'y', source=radar_sitesource, color='black', \n",
    "                 size=8, alpha=0.9, legend_label='Radar Stations')\n",
    "p.circle('x', 'y', source=radar_sitesource, color='blue', \n",
    "                 size=5, alpha=0.2, legend_label='Radar Range',\n",
    "                radius=400000, radius_dimension='max')\n",
    "\n",
    "p.circle('x', 'y', source=sitesource, color='red', \n",
    "                 size=5, alpha=0.3, legend_label='WSC Stations')\n",
    "\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_geodf = gpd.GeoDataFrame(basin_geom,\n",
    "                                crs='EPSG:4326')\n",
    "# convert to Mercator for plotting\n",
    "basin_geo = basin_geodf.to_crs(\"EPSG:3857\")\n",
    "\n",
    "# Get x and y coordinates\n",
    "basin_polygon = list(basin_geo.iloc[0].geometry[0].exterior.coords)\n",
    "\n",
    "basin_df = pd.DataFrame()\n",
    "basin_df['x'] = [e[0] for e in basin_polygon]\n",
    "basin_df['y'] = [e[1] for e in basin_polygon]\n",
    "\n",
    "basin_sitesource = ColumnDataSource(basin_df)\n",
    "\n",
    "l_box, r_box = basin_df['x'].min(), basin_df['x'].max()\n",
    "b_box, t_box = basin_df['y'].min(), basin_df['y'].max()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_stn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(x_range=(l_box*1.0001, 1.0001*r_box), y_range=(0.98*b_box, 1.01*t_box),\n",
    "           x_axis_type=\"mercator\", y_axis_type=\"mercator\",\n",
    "          width=800, height=400)\n",
    "\n",
    "p.add_tile(tile_provider)\n",
    "\n",
    "p.line('x', 'y', source=basin_sitesource, color='blue', \n",
    "     legend_label='WSC 08HB048 Catchment',\n",
    "      line_dash='dashed')\n",
    "\n",
    "p.circle('x', 'y', source=sitesource, color='red', \n",
    "                 size=5, alpha=0.3, legend_label='WSC Stations')\n",
    "\n",
    "p.match_aspect = True\n",
    "# gdf.plot('geometry', ax=ax, color='lightgray', alpha=0.1)\n",
    "# basin_geom.plot(ax=ax, alpha=0.35, edgecolor='k', linewidth=2)\n",
    "# print(stn_df[stn_df['Station Number'] == test_stn])\n",
    "show(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
