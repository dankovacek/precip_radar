{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Query the HYDAT database for runoff events in BC and Alberta that fall within 175km of a radar station in Western Canada.  \n",
    "\n",
    "Filter for watersheds with data after June 2007.  \n",
    "\n",
    "Filter for watersheds larger than 15 km^2 and smaller than 500 km^2.\n",
    "\n",
    "Output table like:\n",
    "\n",
    "| Station | ID | Drainage Area [$km^2$] | Start Date | End Date |\n",
    "|---|---|---|---|---|\n",
    "| Elaho | EHBN008 | 400 | 2007 | 2017 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import sys\n",
    "import math\n",
    "import utm\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import itertools\n",
    "import fiona\n",
    "from geopy import distance\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from shapely.geometry import shape, mapping\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from bokeh.plotting import ColumnDataSource, output_notebook, figure\n",
    "from bokeh.transform import factor_cmap, factor_mark\n",
    "from bokeh.palettes import Spectral3\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.io import show\n",
    "output_notebook()\n",
    "\n",
    "from radar_scrape import get_radar_img_urls, request_img_files\n",
    "from get_station_data import get_daily_runoff\n",
    "from radar_station_coords import radar_sites\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.core import Dense \n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "DB_DIR = os.path.join(BASE_DIR, 'code/hydat_db')\n",
    "PROJECT_DIR = os.path.abspath('')\n",
    "IMG_DIR = os.path.join(PROJECT_DIR, 'data/radar_img')\n",
    "RADAR_IMG_DIR = os.path.join(PROJECT_DIR, 'data/sorted_radar_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_radar_stn(row):\n",
    "    \"\"\" \n",
    "    Input the dict of all station distances,\n",
    "    Return the location code of the nearest radar station.\n",
    "    \"\"\"\n",
    "    radar_station_distances = row['radar_stn_distance_dict']\n",
    "    min_dist = min(radar_station_distances.items(), key=lambda x: x[1])\n",
    "    return min_dist[0]\n",
    "\n",
    "\n",
    "def find_closest_radar_stn_distance(row):\n",
    "    \"\"\" \n",
    "    Input the dict of all station distances,\n",
    "    Return the location code of the nearest radar station.\n",
    "    \"\"\"\n",
    "    radar_station_distances = row['radar_stn_distance_dict']\n",
    "    min_dist = min(radar_station_distances.items(), key=lambda x: x[1])\n",
    "    return min_dist[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(wsc_row, station):\n",
    "    wsc_stn_coords = (wsc_row['Latitude'], wsc_row['Longitude'])\n",
    "    radar_coords = radar_sites[station]['lat_lon']\n",
    "    return distance.distance(radar_coords, wsc_stn_coords).km\n",
    "\n",
    "def calculate_radar_stn_distances(row):\n",
    "    distance_dict = {}\n",
    "    for site in radar_sites:\n",
    "        distance_dict[site] = calc_distance(row, site)\n",
    "    return distance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_radar_stn_distances(row):\n",
    "    distance_dict = {}\n",
    "    for site in radar_sites:\n",
    "        distance_dict[site] = calc_distance(row, site)\n",
    "    return distance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wsc_station_info_dataframe():\n",
    "    # import master station list\n",
    "    stations_df = pd.read_csv(DB_DIR + '/WSC_Stations_Master.csv')\n",
    "    # filter for stations that have concurrent record with the historical radar record\n",
    "    stations_df['RADAR_Overlap'] = stations_df['Year To'].astype(int) - 2007\n",
    "    stations_filtered = stations_df[stations_df['RADAR_Overlap'] > 0]\n",
    "    # filter for stations that are natural flow regimes\n",
    "    stations_filtered = stations_filtered[stations_filtered['Regulation'] == 'N']\n",
    "    stations_filtered.rename(columns={'Gross Drainage Area (km2)': 'DA'}, inplace=True)\n",
    "    # filter for stations in Alberta and British Columbia\n",
    "    stations_filtered = stations_filtered[(stations_filtered['Province'] == 'BC') | (stations_filtered['Province'] == 'AB')]\n",
    "    \n",
    "    # calculate distance to each radar station\n",
    "    stations_filtered['radar_stn_distance_dict'] = stations_filtered.apply(lambda row: calculate_radar_stn_distances(row), axis=1)    \n",
    "    stations_filtered['closest_radar_station'] = stations_filtered.apply(lambda row: find_closest_radar_stn(row), axis=1)\n",
    "    stations_filtered['radar_distance_km'] = stations_filtered.apply(lambda row: find_closest_radar_stn_distance(row), axis=1)\n",
    "    \n",
    "    # radar range is a 240km radius from the station\n",
    "    stations_filtered = stations_filtered[stations_filtered['radar_distance_km'] < 190]\n",
    "    stn_df = stations_filtered[np.isfinite(stations_filtered['DA'].astype(float))]\n",
    "    # filter for stations greater than 10 km^2 (too small for meaningful results)\n",
    "    stn_df = stn_df[stn_df['DA'].astype(float) >= 10]\n",
    "    # filter for stations smaller than 1000 km^2 (too large and complex)\n",
    "    stn_df = stn_df[stn_df['DA'].astype(float) < 1000].sort_values('DA')\n",
    "    df = stn_df[['Province', 'Station Number', 'Station Name', 'DA', \n",
    "                 'Elevation', 'Latitude', 'Longitude', 'RADAR_Overlap',\n",
    "                'closest_radar_station', 'radar_stn_distance_dict', 'radar_distance_km']]\n",
    "#     print('After filtering, there are {} candidate stations.'.format(len(stn_df)))\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_runoff_dataframe(test_stn):\n",
    "    \n",
    "    runoff_df = get_daily_runoff(test_stn)\n",
    "    runoff_df['Year'] = runoff_df.index.year\n",
    "    runoff_df['Month'] = runoff_df.index.month\n",
    "    \n",
    "    # filter by minimum radar date\n",
    "    runoff_df = runoff_df[runoff_df.index > pd.to_datetime('2007-05-31')]\n",
    "    \n",
    "    runoff_df['Date'] = runoff_df.index.values\n",
    "    \n",
    "    return runoff_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_df(df, stn_da):\n",
    "    lag_df = df[['DAILY_FLOW']].copy()\n",
    "    lag_df.rename(columns={'DAILY_FLOW': 'Q'}, inplace=True)\n",
    "\n",
    "    num_lags = int(np.ceil(stn_da / 100) + 5)\n",
    "\n",
    "    for i in range(1,num_lags):\n",
    "        lag_df['Q{}'.format(i)] = lag_df['Q'].shift(i)\n",
    "\n",
    "    lag_df.dropna(inplace=True)\n",
    "    \n",
    "    return lag_df, num_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code from Anomaly detection ML methods article:\n",
    "# https://towardsdatascience.com/machine-learning-for-anomaly-detection-and-condition-monitoring-d4614e7de770\n",
    "def split_train_and_test_data(data, training_months, training_year):\n",
    "    time_range_check = (data.index.year == training_year) & (data.index.month.isin(list(training_months)))\n",
    "    train_data = data[time_range_check]\n",
    "    # the test data is the entire dataset because we want to extract\n",
    "    # extreme events from the training year as well\n",
    "    test_data = data\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):\n",
    "    inv_covariance_matrix = inv_cov_matrix\n",
    "    vars_mean = mean_distr\n",
    "    diff = data - vars_mean\n",
    "    md = []\n",
    "    for i in range(len(diff)):\n",
    "        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MD_detectOutliers(dist, extreme=False, verbose=False):\n",
    "    k = 3. if extreme else 2.\n",
    "    threshold = np.mean(dist) * k\n",
    "    outliers = []\n",
    "    for i in range(len(dist)):\n",
    "        if dist[i] >= threshold:\n",
    "            outliers.append(i)  # index of the outlier\n",
    "    return np.array(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MD_threshold(dist, extreme=False, verbose=False):\n",
    "    k = 3. if extreme else 2.\n",
    "    threshold = np.mean(dist) * k\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(A):\n",
    "    if np.allclose(A, A.T):\n",
    "        try:\n",
    "            np.linalg.cholesky(A)\n",
    "            return True\n",
    "        except np.linalg.LinAlgError:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_PCA(X_train, X_test, n_components):\n",
    "    \n",
    "    for n_components_kept in range(2, n_components + 1):\n",
    "\n",
    "        pca = PCA(n_components=n_components_kept, svd_solver= 'full')\n",
    "        X_train_PCA = pca.fit_transform(X_train)\n",
    "        X_train_PCA = pd.DataFrame(X_train_PCA)\n",
    "        X_train_PCA.index = X_train.index\n",
    "\n",
    "        X_test_PCA = pca.transform(X_test)\n",
    "        X_test_PCA = pd.DataFrame(X_test_PCA)\n",
    "        X_test_PCA.index = X_test.index\n",
    "\n",
    "        var_expl = 100*np.sum(pca.explained_variance_ratio_)\n",
    "        if var_expl >= 90:\n",
    "#             print('var > 0.9 in {} components'.format(n_components_kept))\n",
    "            return X_train_PCA, X_test_PCA, var_expl, n_components_kept\n",
    "#     print('var < 0.9 in {} components'.format(n_components_kept))\n",
    "    return X_train_PCA, X_test_PCA, var_expl, n_components_kept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_input_data(wsc_stn_num):\n",
    "        \n",
    "    t0 = time.time()\n",
    "    stn_df = initialize_wsc_station_info_dataframe()\n",
    "\n",
    "    test_stn_info = stn_df[stn_df['Station Number'] == wsc_stn_num]\n",
    "    stn_da = test_stn_info['DA'].values[0]\n",
    "    wsc_stn_name = test_stn_info['Station Name'].values[0]\n",
    "    closest_radar_stn = test_stn_info['closest_radar_station'].values[0]\n",
    "#     print('{} ({}) has a DA of {} km^2'.format(wsc_stn_name, wsc_stn_num, stn_da))\n",
    "    \n",
    "    runoff_df = initialize_runoff_dataframe(wsc_stn_num)    \n",
    "    lag_df, num_lags = create_lag_df(runoff_df, stn_da) \n",
    "    \n",
    "    \n",
    "    candidate_stations = stn_df['Station Number'].values\n",
    "    \n",
    "    return lag_df, closest_radar_stn, runoff_df, num_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_from_annual_distribution(df):\n",
    "    annual_dist = df.groupby(df.index.month).mean()\n",
    "#     print(annual_dist)\n",
    "    annual_dist['rank'] = annual_dist['DAILY_FLOW'].rank(ascending=False)\n",
    "    annual_dist['b'] = 1 - annual_dist['rank'] / float(len(annual_dist) - 1)\n",
    "    june_val = annual_dist[annual_dist.index == 6]['b'].values[0]\n",
    "    if june_val > 0.8:\n",
    "        return 7\n",
    "    else:\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_data):\n",
    "    tstart = time.time()\n",
    "    training_months = list(input_data['months'])\n",
    "    training_year = input_data['year']\n",
    "    wsc_station_num = input_data['wsc_stn']\n",
    "    training_sample_size = input_data['n_sample']\n",
    "    stn_da = input_data['stn_da']\n",
    "    closest_radar_stn = input_data['radar_stn']\n",
    "    \n",
    "    lag_df = input_data['lag_df']\n",
    "    num_lags = input_data['num_lags']\n",
    "    runoff_df = input_data['runoff_df']\n",
    "    \n",
    "    dataset_train, dataset_test = split_train_and_test_data(lag_df, training_months, training_year)\n",
    "    \n",
    "    training_set_len = len(dataset_train)\n",
    "    \n",
    "    if len(dataset_train) < 25:\n",
    "#         print('exited because dataset_train is too small')\n",
    "#         print(dataset_train)\n",
    "        return pd.DataFrame([]), 0\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(dataset_train), \n",
    "                                  columns=dataset_train.columns, \n",
    "                                  index=dataset_train.index)\n",
    "    # Random shuffle training data\n",
    "    X_train.sample(frac=1)\n",
    "\n",
    "    X_test = pd.DataFrame(scaler.transform(dataset_test), \n",
    "                                 columns=dataset_test.columns, \n",
    "                                 index=dataset_test.index)\n",
    "    t1 = time.time()\n",
    "    \n",
    "   \n",
    "    X_train_PCA, X_test_PCA, var_expl, n_components = do_PCA(X_train, X_test, num_lags)\n",
    "    \n",
    "    data_train = np.array(X_train_PCA.values)\n",
    "    data_test = np.array(X_test_PCA.values)\n",
    "    t2 = time.time()\n",
    "#     print('time to end of PCA = {:.4f}'.format(t2-tstart))\n",
    "\n",
    "    \n",
    "    def cov_matrix(data, verbose=False):\n",
    "        covariance_matrix = np.cov(data, rowvar=False)\n",
    "        if is_pos_def(covariance_matrix):\n",
    "            inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n",
    "            if is_pos_def(inv_covariance_matrix):\n",
    "                return True, covariance_matrix, inv_covariance_matrix\n",
    "            else:\n",
    "                print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n",
    "                return False, None, None\n",
    "        else:\n",
    "#             print(\"Error: Covariance Matrix is not positive definite!\")\n",
    "            return False, None, None\n",
    "\n",
    "               \n",
    "    cov_test, cov_matrix, inv_cov_matrix = cov_matrix(data_train)\n",
    "    \n",
    "    if cov_test == False:\n",
    "        return pd.DataFrame([]), 0\n",
    "\n",
    "    mean_distr = data_train.mean(axis=0)\n",
    "\n",
    "    dist_test = MahalanobisDist(inv_cov_matrix, mean_distr, data_test, verbose=False)\n",
    "    dist_train = MahalanobisDist(inv_cov_matrix, mean_distr, data_train, verbose=False)\n",
    "    threshold = MD_threshold(dist_train, extreme = True)\n",
    "    \n",
    "    anomaly_train = pd.DataFrame()\n",
    "    anomaly_train['Mob dist']= dist_train\n",
    "    anomaly_train['Thresh'] = threshold\n",
    "    # If Mob dist above threshold: Flag as anomaly\n",
    "    anomaly_train['Anomaly'] = anomaly_train['Mob dist'] > anomaly_train['Thresh']\n",
    "    anomaly_train.index = X_train_PCA.index\n",
    "    anomaly = pd.DataFrame()\n",
    "    anomaly['Mob dist']= dist_test\n",
    "    anomaly['Thresh'] = threshold\n",
    "    anomaly['num_components_kept'] = n_components\n",
    "    # If Mob dist above threshold: Flag as anomaly\n",
    "    anomaly['Anomaly'] = anomaly['Mob dist'] > anomaly['Thresh']\n",
    "    anomaly.index = X_test_PCA.index\n",
    "    anomaly.head()\n",
    "    \n",
    "    anomaly_alldata = pd.concat([anomaly_train, anomaly], sort=True)\n",
    "    \n",
    "    event_times = np.where(anomaly_alldata['Anomaly'].values[:-1] != anomaly_alldata['Anomaly'].values[1:])[0]\n",
    "    events = pd.merge(lag_df, anomaly_alldata.iloc[event_times,:], how='inner', \n",
    "                      left_index=True, right_index=True)\n",
    "\n",
    "    events = events.loc[~events.index.duplicated(keep='first')]\n",
    "    \n",
    "    if len(events) < 5:\n",
    "#         print('exited because len(events) < 5')\n",
    "        return pd.DataFrame([]), n_components\n",
    "    elif events.iloc[0]['Anomaly'] == True:\n",
    "        events = events.iloc[1:]\n",
    "        \n",
    "    # create a column of time difference between events in days\n",
    "    events['dt_days'] = events.index.to_series().diff(1)    \n",
    "\n",
    "    a = time.time()\n",
    "\n",
    "    last_event_end = False\n",
    "\n",
    "    new_events = pd.DataFrame()\n",
    "\n",
    "    # iterate through the detected event pairs \n",
    "    for i in np.arange(0, len(events) - 1, 2):\n",
    "        \n",
    "        # parse a single event pair\n",
    "        this_event = events.iloc[i:i+2]\n",
    "        \n",
    "        check_sign_switch = this_event['Anomaly'].values[0] != this_event['Anomaly'].values[1]\n",
    "        taa = time.time()\n",
    "        concurrent_wsc = lag_df[(lag_df.index >= this_event.index.values[0]) & (lag_df.index <= this_event.index.values[1])][['Q']]\n",
    "        peak_in_middle = check_peak_in_middle(this_event, concurrent_wsc)\n",
    "\n",
    "\n",
    "        if (check_sign_switch) & (peak_in_middle):\n",
    "            \n",
    "            \n",
    "            \n",
    "            # get the start date\n",
    "            this_event_start = pd.to_datetime(this_event[this_event['Anomaly'] == False].index.values[0])\n",
    "            # get the end date\n",
    "            this_event_end = pd.to_datetime(this_event[this_event['Anomaly'] == True].index.values[0])\n",
    "            tloops = time.time()\n",
    "            adjusted_start_date = pd.to_datetime(adjust_edge_date(this_event_start, lag_df[['Q']], 'start', stn_da))\n",
    "            adjusted_end_date = pd.to_datetime(adjust_edge_date(this_event_end, lag_df[['Q']], 'end', stn_da))\n",
    "            \n",
    "            lag_df_start = pd.to_datetime(lag_df.index.values[0])\n",
    "            \n",
    "            tin = time.time()\n",
    "#             print('asd {:.3f}'.format(tin - tloops))\n",
    "            \n",
    "            # check if the adjusted start date predates the record\n",
    "            if lag_df_start > adjusted_start_date:\n",
    "                adjusted_start_date = lag_df_start\n",
    "#                 print('this was adjusted')\n",
    "            \n",
    "            if last_event_end is not False:\n",
    "                # find if the start date is on the rising limb - adjust if so\n",
    "\n",
    "                if adjusted_start_date < last_event_end:\n",
    "                    adjusted_start_date = last_event_end + pd.DateOffset(1)\n",
    "                    \n",
    "            new_event_start = lag_df[lag_df.index == adjusted_start_date][['Q']]\n",
    "            new_event_end = lag_df[lag_df.index == adjusted_end_date][['Q']] \n",
    "\n",
    "            new_event_start['timing'] = 'start'\n",
    "            new_event_end['timing'] = 'end'\n",
    "            \n",
    "            start_month_limit = get_start_from_annual_distribution(runoff_df)            \n",
    "            \n",
    "            if stn_da < 100:\n",
    "                max_days = 4\n",
    "            elif stn_da < 500:\n",
    "                max_days = 6\n",
    "            else:\n",
    "                max_days = 14\n",
    "                \n",
    "            if len(new_event_start) == 0:\n",
    "                new_event_start = lag_df[lag_df.index == this_event_start][['Q']]\n",
    "            if len(new_event_end) == 0:\n",
    "                new_event_end = lag_df[lag_df.index == this_event_end][['Q']]\n",
    "            \n",
    "            min_time_check = (new_event_end.index - new_event_start.index).days > 1\n",
    "            max_time_check = (new_event_end.index - new_event_start.index).days <= max_days\n",
    "            start_month = new_event_start.index.month\n",
    "            \n",
    "            end_month = new_event_end.index.month\n",
    "            season_check = (start_month > 5) & (start_month < 11) & (end_month <= 11)\n",
    "\n",
    "            if (min_time_check) & (max_time_check) & (season_check):\n",
    "                new_events = new_events.append(new_event_start, sort=True)\n",
    "                new_events = new_events.append(new_event_end, sort=True)\n",
    "\n",
    "            last_event_end = pd.to_datetime(this_event_end)\n",
    "            \n",
    "\n",
    "    new_events.sort_index(inplace=True)    \n",
    "\n",
    "    new_events['dt_days'] = new_events.index.to_series().diff(1)\n",
    "    new_events['wsc_station'] = wsc_station_num\n",
    "    new_events['training_year'] = training_year\n",
    "    new_events['training_months'] = str(training_months)# for e in new_events]\n",
    "    new_events['training_set_len'] = training_set_len\n",
    "    new_events['m_threshold'] = threshold\n",
    "    new_events['var_explained'] = var_expl\n",
    "    new_events['n_components'] = n_components\n",
    "    new_events['num_lags'] = num_lags\n",
    "    new_events['radar_stn'] = closest_radar_stn\n",
    "                \n",
    "    return new_events, n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_edge_date(initial_date, data, direction, stn_da):\n",
    "    \"\"\"\n",
    "    If the start flow is on a rising limb, adjust the start to the start of the runoff event.\n",
    "    \"\"\"\n",
    "    initial_val = data[data.index == initial_date]['Q']\n",
    "\n",
    "    \n",
    "    if direction == 'start':\n",
    "        search_criteria = (data.index >= initial_date - pd.Timedelta('7 days')) & (data.index <= initial_date)\n",
    "        search_direction = 1\n",
    "    elif direction == 'end':\n",
    "        search_criteria = (data.index <= initial_date + pd.Timedelta('3 days')) & (data.index >= initial_date)\n",
    "        search_direction = 1\n",
    "\n",
    "        \n",
    "    extended_week_vals = data[search_criteria][['Q']]\n",
    "    extended_week_vals['diff'] = extended_week_vals.diff(periods=search_direction)\n",
    "    extended_week_vals['pct_change'] = 100 * extended_week_vals['diff'] / extended_week_vals['Q']\n",
    "\n",
    "    if direction == 'start':\n",
    "        try:\n",
    "            change_date = pd.to_datetime(extended_week_vals[['Q']].idxmin().values[0])\n",
    "            change_point_date = change_date - pd.DateOffset(1)\n",
    "            adjusted_date = change_point_date\n",
    "            \n",
    "        except ValueError as err:\n",
    "            adjusted_date = initial_date\n",
    "\n",
    "    elif direction == 'end':\n",
    "        try:\n",
    "            adjusted_date = pd.to_datetime(extended_week_vals[['diff']].idxmin().values[0])\n",
    "        except ValueError as err:\n",
    "            print('print error in adjusting event end date', err)\n",
    "            adjusted_date = initial_date\n",
    "            \n",
    "    return adjusted_date\n",
    "\n",
    "\n",
    "def check_peak_in_middle(event, data):\n",
    "    \"\"\"\n",
    "    Ensure there is a peak between the start and end points\n",
    "    so we aren't targeting a non-runoff event.\n",
    "    \"\"\"\n",
    "    start_time = event.index.values[0] \n",
    "    end_time = event.index.values[-1]\n",
    "    max_time = data[data['Q'] == data['Q'].max()].index.values[0]\n",
    "    if (max_time == start_time) | (max_time == end_time):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def get_all_combinations(months, years):\n",
    "    month_combos = [list(itertools.combinations(months, n)) for n in list(range(1, 13))]\n",
    "    flat_combos =  [item for sublist in month_combos for item in sublist]\n",
    "    return np.asarray(list(itertools.product(flat_combos, years)))\n",
    "\n",
    "def calc_softmax(X):\n",
    "    return np.exp(X) / np.sum(np.exp(X))\n",
    "\n",
    "def filter_input_data(data):\n",
    "    filtered = []\n",
    "    for d in data:\n",
    "        months = list(d[0])\n",
    "        common_months = [m for m in months if m in [7, 8, 9, 10]]\n",
    "        if (len(months) == 1) & (months[0] not in [12, 1, 2, 3]):\n",
    "            filtered.append(list(d))\n",
    "        elif (len(months) > 1) & (len(common_months) > 0):\n",
    "            filtered.append(list(d))\n",
    "    return filtered\n",
    "\n",
    "def run_AD_training(wsc_station_num, training_sample_size=5):\n",
    "    \n",
    "    radar_stn = stn_df[stn_df['Station Number'] == wsc_station_num]['closest_radar_station'].values[0]\n",
    "    stn_da = stn_df[stn_df['Station Number'] == wsc_station_num]['DA'].values[0]\n",
    "    \n",
    "    lag_df, closest_radar_stn, runoff_df, num_lags = initialize_input_data(wsc_station_num)\n",
    "    \n",
    "#     runoff_df = initialize_runoff_dataframe(wsc_station_num)  \n",
    "    \n",
    "    training_months = list(set(runoff_df.index.month))\n",
    "    training_years = list(set(runoff_df.index.year))\n",
    "\n",
    "    all_combinations = get_all_combinations(training_months, training_years)\n",
    "    \n",
    "    filtered_combinations = np.array(filter_input_data(all_combinations))\n",
    "        \n",
    "    weights = calc_softmax([(13.0 - len(c[0]))*3.5 for c in filtered_combinations])\n",
    "    \n",
    "    # a complete search is intractable, so sample n permutations without replacement\n",
    "    rand_ints = np.random.choice(range(len(filtered_combinations)), training_sample_size, \n",
    "                                 replace=False, p=weights)\n",
    "    \n",
    "    initial_pop = [filtered_combinations[i] for i in rand_ints]\n",
    "    \n",
    "\n",
    "    input_array = []\n",
    "    for combo in initial_pop:\n",
    "        input_data = {'year': combo[1],\n",
    "                     'months': combo[0],\n",
    "                     'n_sample': training_sample_size,\n",
    "                     'wsc_stn': wsc_station_num,\n",
    "                     'stn_da': stn_da,\n",
    "                     'radar_stn': radar_stn,\n",
    "                      'lag_df': lag_df,\n",
    "                      'num_lags': num_lags,\n",
    "                      'runoff_df': runoff_df\n",
    "                     }\n",
    "        input_array.append(input_data)\n",
    "        \n",
    "    results = []\n",
    "    for input_dat in input_array:\n",
    "        result, n_components = train_model(input_dat)\n",
    "        results.append((input_dat, n_components, result))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.888888888888888"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "124 * 200 / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "84 of 147: 08NE110: 60.26s\n",
      "85 of 147: 08HD011: 143.55s\n",
      "86 of 147: 07EE009: 61.60s\n",
      "87 of 147: 08LG048: 95.47s\n",
      "88 of 147: 05CK007: 4.04s\n",
      "89 of 147: 05BG006: 94.52s\n",
      "90 of 147: 08NE039: 78.06s\n",
      "91 of 147: 05AC030: 64.63s\n",
      "92 of 147: 05CC008: 62.50s\n",
      "93 of 147: 08MG001: 135.59s\n",
      "94 of 147: 08NL050: 70.65s\n",
      "95 of 147: 08KA001: 42.25s\n",
      "96 of 147: 08NF001: 67.68s\n",
      "97 of 147: 08NB014: 77.69s\n",
      "98 of 147: 08JE004: 71.65s\n",
      "99 of 147: 08NH005: 73.64s\n",
      "100 of 147: 05CB002: 67.18s\n",
      "101 of 147: 05CC011: 79.40s\n",
      "102 of 147: 05CA012: 65.13s\n",
      "103 of 147: 05CE012: 31.85s\n",
      "104 of 147: 05BL013: 59.48s\n",
      "105 of 147: 05CD007: 61.82s\n",
      "106 of 147: 08KH019: 64.33s\n",
      "107 of 147: 08HA010: 167.59s\n",
      "108 of 147: 08NH130: 66.79s\n",
      "109 of 147: 05BL014: 77.90s\n",
      "110 of 147: 08NK018: 79.23s\n",
      "111 of 147: 08LB069: 54.63s\n",
      "112 of 147: 05CK001: 68.94s\n",
      "113 of 147: 05CE006: 58.41s\n",
      "114 of 147: 05BL019: 55.19s\n",
      "115 of 147: 08LG008: 40.85s\n",
      "116 of 147: 05BM014: 58.54s\n",
      "117 of 147: 05BJ004: 69.99s\n",
      "118 of 147: 05CC013: 46.89s\n",
      "119 of 147: 08LE027: 89.16s\n",
      "120 of 147: 05CA002: 57.00s\n",
      "121 of 147: 05DB002: 81.44s\n",
      "122 of 147: 08ND012: 77.57s\n",
      "123 of 147: 05CA004: 58.91s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-01c341adf9d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For n={}, best result = {} in = {}s'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ta' is not defined"
     ]
    }
   ],
   "source": [
    "stn_df = initialize_wsc_station_info_dataframe()\n",
    "\n",
    "bad_stns = [\n",
    "    '08NE006', '05AB005', '08LE024', '05AB005', '05BL027', '08MF068', '08MF065',\n",
    "    '05CB004', '08HA001', '05FC002', '08NL069', '08MH103', '08NL070', '08HB002',\n",
    "    '08MH016', '08MG026', '08LG068', '08MH056', '08ME027', '08MF062', '08HB048',\n",
    "    '08HB032', '08LE077', '08HA016', '08LF100',\n",
    "]\n",
    "\n",
    "all_wsc_stations = stn_df['Station Number'].values\n",
    "\n",
    "filtered_wsc_stns = [e for e in all_wsc_stations if e not in bad_stns]\n",
    "\n",
    "print(len(filtered_wsc_stns))\n",
    "\n",
    "best_results = {}\n",
    "for s_size in [200]:\n",
    "    sample_path = os.path.join(PROJECT_DIR, 'data/AD_results/sample_{}/'.format(s_size))\n",
    "    \n",
    "    if not os.path.exists(sample_path):\n",
    "        os.makedirs(sample_path)\n",
    "    \n",
    "    n = 83\n",
    "    ta = time.time()\n",
    "    for wsc_stn in filtered_wsc_stns[84:]:\n",
    "        \n",
    "        n += 1\n",
    "        t0 = time.time()\n",
    "        results = run_AD_training(wsc_stn, s_size)\n",
    "        t1 = time.time()\n",
    "        print('{} of {}: {}: {:.2f}s'.format(n, len(all_wsc_stations), wsc_stn, t1 - t0))\n",
    "        AD_model_params = []\n",
    "        for r in results:\n",
    "            input_data = r[0]\n",
    "            n_components = r[1]\n",
    "            result_info = r[2]\n",
    "#             print(params, len(result_info), num_lags)\n",
    "            months = input_data['months']\n",
    "            year = input_data['year']\n",
    "            n_sample = input_data['n_sample']\n",
    "            radar_stn = input_data['radar_stn']\n",
    "            AD_model_params.append((months, year, wsc_stn, radar_stn, n_sample, len(result_info), n_components))\n",
    "\n",
    "        result_df = pd.DataFrame(AD_model_params, columns=['train_months', 'train_year', 'wsc_stn', 'radar_stn', 'n_sample', 'len_results', 'num_components'])\n",
    "        results_save_path = os.path.join(PROJECT_DIR, 'data/AD_results/sample_{}/{}_results.csv'.format(n_sample, wsc_stn))\n",
    "        result_df.to_csv(results_save_path)\n",
    "        \n",
    "#         print(result_df)\n",
    "        \n",
    "        best_result = result_df.sort_values(by='len_results', ascending=False).iloc[0]\n",
    "        best_results[wsc_stn] = best_result\n",
    "        \n",
    "    tb = time.time()\n",
    "    print('For n={}, best result = {} in = {}s'.format(s_size, best_result.loc['len_results'], tb-ta))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  End of Find Events Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_AD_training(wsc_station_num, stn_df, runoff_df, radar_stn, training_sample_size=5, ):\n",
    "  \n",
    "    input_array = [[*c, training_sample_size, wsc_station_num, radar_stn] for c in sample_list]\n",
    "    \n",
    "    results = train_model(input_array)\n",
    "    \n",
    "    return results\n",
    "\n",
    "stn = list(best_results.keys())[0]\n",
    "print(stn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASAG\n",
      "(                    Q         Q1         Q2         Q3         Q4         Q5  \\\n",
      "DATE                                                                           \n",
      "2007-06-08  47.599998  57.099998  69.699997  70.800003  69.199997  55.900002   \n",
      "2007-06-09  44.700001  47.599998  57.099998  69.699997  70.800003  69.199997   \n",
      "2007-06-10  46.099998  44.700001  47.599998  57.099998  69.699997  70.800003   \n",
      "2007-06-11  46.500000  46.099998  44.700001  47.599998  57.099998  69.699997   \n",
      "2007-06-12  41.000000  46.500000  46.099998  44.700001  47.599998  57.099998   \n",
      "...               ...        ...        ...        ...        ...        ...   \n",
      "2017-12-27   4.570000   4.640000   4.720000   4.780000   5.000000   5.340000   \n",
      "2017-12-28   4.560000   4.570000   4.640000   4.720000   4.780000   5.000000   \n",
      "2017-12-29   4.530000   4.560000   4.570000   4.640000   4.720000   4.780000   \n",
      "2017-12-30   4.620000   4.530000   4.560000   4.570000   4.640000   4.720000   \n",
      "2017-12-31   4.410000   4.620000   4.530000   4.560000   4.570000   4.640000   \n",
      "\n",
      "                   Q6         Q7  \n",
      "DATE                              \n",
      "2007-06-08  51.400002  46.700001  \n",
      "2007-06-09  55.900002  51.400002  \n",
      "2007-06-10  69.199997  55.900002  \n",
      "2007-06-11  70.800003  69.199997  \n",
      "2007-06-12  69.699997  70.800003  \n",
      "...               ...        ...  \n",
      "2017-12-27   5.630000   5.800000  \n",
      "2017-12-28   5.340000   5.630000  \n",
      "2017-12-29   5.000000   5.340000  \n",
      "2017-12-30   4.780000   5.000000  \n",
      "2017-12-31   4.720000   4.780000  \n",
      "\n",
      "[3860 rows x 8 columns], 'CASAG',             DAILY_FLOW FLAG_08GA072  Year  Month       Date\n",
      "DATE                                                       \n",
      "2007-06-01   46.700001         None  2007      6 2007-06-01\n",
      "2007-06-02   51.400002         None  2007      6 2007-06-02\n",
      "2007-06-03   55.900002         None  2007      6 2007-06-03\n",
      "2007-06-04   69.199997         None  2007      6 2007-06-04\n",
      "2007-06-05   70.800003         None  2007      6 2007-06-05\n",
      "...                ...          ...   ...    ...        ...\n",
      "2017-12-27    4.570000         None  2017     12 2017-12-27\n",
      "2017-12-28    4.560000         None  2017     12 2017-12-28\n",
      "2017-12-29    4.530000         None  2017     12 2017-12-29\n",
      "2017-12-30    4.620000         None  2017     12 2017-12-30\n",
      "2017-12-31    4.410000         None  2017     12 2017-12-31\n",
      "\n",
      "[3867 rows x 5 columns], 8)\n"
     ]
    }
   ],
   "source": [
    "# train_year = best_results[stn]['train_year']\n",
    "# radar_stn = best_results[stn]['radar_stn']\n",
    "# training_months = best_results[stn]['train_months']\n",
    "# training_sample_size = best_results[stn]['n_sample']\n",
    "\n",
    "# input_array = [training_months, train_year, training_sample_size, stn, radar_stn]\n",
    "\n",
    "# best_events, n_components = train_model(input_array)\n",
    "stn = '08GA072'\n",
    "lag_df, closest_radar_stn, runoff_df, num_lags = initialize_input_data(stn)\n",
    "print(closest_radar_stn)\n",
    "stn_df = initialize_input_data(stn)\n",
    "print(stn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(best_events))\n",
    "# print(best_events)\n",
    "stn = '08GA072'\n",
    "print(stn)\n",
    "# print(stn_df)\n",
    "print(stn_df[stn_df['Station Number'] == stn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid plot of individual events\n",
    "\n",
    "plots = []\n",
    "\n",
    "for i in np.arange(0, len(best_events) - 1, 2):\n",
    "    \n",
    "    # parse a single event pair\n",
    "    this_event = best_events.iloc[i:i+2]\n",
    "    \n",
    "    s1 = figure(background_fill_color=\"#fafafa\", x_axis_type='datetime')\n",
    "    \n",
    "    s1.circle(this_event.index, this_event['Q'], \n",
    "              size=12, alpha=0.8, color=\"red\")#, legend_label='{estimated endpoints}')\n",
    "    s1.xaxis.major_label_orientation = math.pi / 2\n",
    "#     s1.yaxis.axis_label = 'Flow [cms]'\n",
    "    this_start = pd.to_datetime(this_event.index.values[0])\n",
    "    this_end = pd.to_datetime(this_event.index.values[1])\n",
    "    this_dat = lag_df[(lag_df.index >= this_start) & (lag_df.index <= this_end)][['Q']]\n",
    "    \n",
    "    if (this_end.month < 12) & (this_start.month > 5):\n",
    "        year = this_event.index.year.values[0]\n",
    "        month = this_event.index.month.values[0]\n",
    "        day = this_event.index.day.values[0]\n",
    "        date = '{}-{}-{}'.format(year, month, day)\n",
    "        s1.line(this_dat.index, this_dat['Q'], color='blue')\n",
    "        plots.append(s1)\n",
    "\n",
    "print('there are {} plots'.format(len(plots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(plots) < 6:\n",
    "#     grid = gridplot(plots, plot_width=150, plot_height=150)\n",
    "# else:\n",
    "n_cols = 8\n",
    "n_rows = int(np.ceil(len(plots) / n_cols))\n",
    "\n",
    "g = []\n",
    "for i in range(0, len(plots), n_cols):\n",
    "    this_plot = plots[i]\n",
    "    if i % n_cols == 0:\n",
    "        print(i)\n",
    "        this_plot.yaxis.axis_label = 'Flow [cms]'\n",
    "    g += [plots[i:i+n_cols]]\n",
    "grid = gridplot(g, plot_width=150, plot_height=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_pairs = []\n",
    "for i in np.arange(0, len(best_events) - 1, 2):\n",
    "    # parse a single event pair\n",
    "    this_event = best_events.iloc[i:i+2]\n",
    "    date_pair = [e.astype(str).replace('T', ' ').split('.')[0].split(' ')[0] for e in this_event.index.values]\n",
    "    this_start = pd.to_datetime(this_event.index.values[0])\n",
    "    this_month = this_start.month\n",
    "    if (this_month > 5) & (this_month <= 11) & (this_start > pd.to_datetime('2007-01-01')):\n",
    "        event_pairs.append(date_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_events.iloc[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop winter events and group for individual plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = best_events[best_events['timing'] == 'start']\n",
    "ends = best_events[best_events['timing'] == 'end']\n",
    "runoff_df = initialize_runoff_dataframe(stn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=800, plot_height=400, x_axis_type='datetime')\n",
    "\n",
    "# p.circle(adj_starts.index, adj_starts['Q'], size=10, color=\"red\", \n",
    "#          alpha=0.5, legend_label='start'.format(len(foo)))\n",
    "# p.circle(adj_ends.index, adj_ends['Q'], size=10, color=\"blue\", \n",
    "#          alpha=0.5, legend_label='end'.format(len(foo)))\n",
    "\n",
    "p.circle(starts.index, starts['Q'], size=10, color=\"red\", \n",
    "         alpha=0.5, legend_label='start')\n",
    "p.circle(ends.index, ends['Q'], size=10, color=\"blue\", \n",
    "         alpha=0.5, legend_label='end')\n",
    "# p.line(input_sig.index, input_sig['f_sig'], color='blue')\n",
    "p.line(runoff_df.index, runoff_df['DAILY_FLOW'], color='blue')\n",
    "\n",
    "p.yaxis.axis_label = 'Flow [cms]'\n",
    "# p.line()\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autoencoder Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(10)\n",
    "tensorflow.random.set_seed(10)\n",
    "act_func = 'elu'\n",
    "\n",
    "# Input layer:\n",
    "model=Sequential()\n",
    "# First hidden layer, connected to input vector X. \n",
    "model.add(Dense(10,activation=act_func,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.0),\n",
    "                input_shape=(X_train.shape[1],)\n",
    "               )\n",
    "         )\n",
    "\n",
    "model.add(Dense(2,activation=act_func,\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(10,activation=act_func,\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(X_train.shape[1],\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "# Train model for 100 epochs, batch size of 10: \n",
    "NUM_EPOCHS=100\n",
    "BATCH_SIZE=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(np.array(X_train),np.array(X_train),\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  epochs=NUM_EPOCHS,\n",
    "                  validation_split=0.05,\n",
    "                  verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],\n",
    "         'b',\n",
    "         label='Training loss')\n",
    "plt.plot(history.history['val_loss'],\n",
    "         'r',\n",
    "         label='Validation loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss, [mse]')\n",
    "plt.ylim([0,.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(np.array(X_train))\n",
    "X_pred = pd.DataFrame(X_pred, \n",
    "                      columns=X_train.columns)\n",
    "X_pred.index = X_train.index\n",
    "\n",
    "scored = pd.DataFrame(index=X_train.index)\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)\n",
    "plt.figure()\n",
    "sns.distplot(scored['Loss_mae'],\n",
    "             bins = 10, \n",
    "             kde= True,\n",
    "            color = 'blue');\n",
    "plt.xlim([0.0,.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(np.array(X_test))\n",
    "X_pred = pd.DataFrame(X_pred, \n",
    "                      columns=X_test.columns)\n",
    "X_pred.index = X_test.index\n",
    "\n",
    "scored = pd.DataFrame(index=X_test.index)\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_test), axis = 1)\n",
    "scored['Threshold'] = 0.3\n",
    "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_train = model.predict(np.array(X_train))\n",
    "X_pred_train = pd.DataFrame(X_pred_train, \n",
    "                      columns=X_train.columns)\n",
    "X_pred_train.index = X_train.index\n",
    "\n",
    "scored_train = pd.DataFrame(index=X_train.index)\n",
    "scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-X_train), axis = 1)\n",
    "scored_train['Threshold'] = 0.3\n",
    "scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\n",
    "scored = pd.concat([scored_train, scored], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored[scored.index > pd.to_datetime('2016-06-01')].plot(logy=True,  figsize = (10,6), ylim = [1e-2,1e2], color = ['blue','red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_times = np.where(scored['Anomaly'].values[:-1] != scored['Anomaly'].values[1:])[0]\n",
    "events = pd.merge(input_sig, scored.iloc[event_times,:], how='inner', \n",
    "                  left_index=True, right_index=True)\n",
    "\n",
    "starts = events[events['Anomaly'] == False]\n",
    "ends = events[events['Anomaly'] == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=800, plot_height=400, x_axis_type='datetime')\n",
    "\n",
    "# p.circle(adj_starts.index, adj_starts['Q'], size=10, color=\"red\", \n",
    "#          alpha=0.5, legend_label='start'.format(len(foo)))\n",
    "# p.circle(adj_ends.index, adj_ends['Q'], size=10, color=\"blue\", \n",
    "#          alpha=0.5, legend_label='end'.format(len(foo)))\n",
    "\n",
    "p.circle(starts.index, starts['DAILY_FLOW'], size=10, color=\"red\", \n",
    "         alpha=0.5, legend_label='start'.format(len(foo)))\n",
    "p.circle(ends.index, ends['DAILY_FLOW'], size=10, color=\"blue\", \n",
    "         alpha=0.5, legend_label='end'.format(len(foo)))\n",
    "# p.line(input_sig.index, input_sig['f_sig'], color='blue')\n",
    "p.line(input_sig.index, input_sig['DAILY_FLOW'], color='blue')\n",
    "# p.line()\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates_covered = []\n",
    "# fldr = os.path.join(IMG_DIR, test_stn)\n",
    "# for f in os.listdir(fldr):\n",
    "#     date = f[:4] + '-' + f[4:6] + '-' + f[6:8]\n",
    "#     dates_covered.append(date)\n",
    "\n",
    "# dates_covered = list(set(dates_covered))\n",
    "# unchecked = []\n",
    "# for ep in event_pairs:\n",
    "#     if (ep[0] not in dates_covered) & (ep[1] not in dates_covered):\n",
    "#         unchecked.append(ep)\n",
    "        \n",
    "# print(unchecked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks(data, lag=7, threshold=500, influence=0.5):\n",
    "    # Settings (the ones below are examples: choose what is best for your data)\n",
    "#     lag = 5         # lag 5 for the smoothing functions\n",
    "#     threshold = 3.5  # 3.5 standard deviations for signal\n",
    "#     influence = 0.5  # between 0 and 1, where 1 is normal influence, 0.5 is half\n",
    "    # Initialize variables\n",
    "    signals = np.zeros(len(data))            # Initialize signal results\n",
    "    filteredY = np.empty(len(data))\n",
    "    filteredY[:lag] = data[:lag]             # Initialize filtered series\n",
    "    avgFilter = [0]                          # Initialize average filter\n",
    "    stdFilter = [0]                          # Initialize std. filter\n",
    "    avgFilter = {lag: np.mean(data[:lag])}      # Initialize first value\n",
    "    stdFilter = {lag: np.std(data[:lag])}     # Initialize first value\n",
    "    \n",
    "    for i in range(lag + 1, len(data)):\n",
    "        d = data[i]\n",
    "        \n",
    "        af = avgFilter[i-1]\n",
    "        sf = stdFilter[i-1]\n",
    "        \n",
    "        if abs(d - af) > threshold * sf:\n",
    "            if d > af:\n",
    "                signals[i] = 1                     # Positive signal\n",
    "            else:\n",
    "                signals[i] = -1                    # Negative signal\n",
    "\n",
    "            \n",
    "            filteredY[i] = influence*d + (1-influence)*filteredY[i-1]\n",
    "        else:\n",
    "            signals[i] = 0                        # No signal\n",
    "            filteredY[i] = 0\n",
    "        \n",
    "        \n",
    "        # Adjust the filters\n",
    "        avgFilter[i] = np.mean(filteredY[i-lag:i])\n",
    "        stdFilter[i] = np.std(filteredY[i-lag:i])\n",
    "        \n",
    "    return signals, filteredY\n",
    "\n",
    "n_test = 500\n",
    "\n",
    "dats = list(df['DAILY_FLOW'].to_numpy())\n",
    "sigs, f_dat = find_peaks(dats, influence=0.75, lag=7, threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show, output_notebook\n",
    "\n",
    "input_sig = df[['DAILY_FLOW']].copy()\n",
    "signal = np.array(sigs)\n",
    "input_sig['sig'] = signal.copy().astype(int)\n",
    "input_sig['f_sig'] = f_dat\n",
    "\n",
    "foo = input_sig[input_sig['sig'] == 1].copy()\n",
    "p = figure(plot_width=800, plot_height=400, x_axis_type='datetime')\n",
    "# add a circle renderer with a size, color, and alpha\n",
    "p.circle(foo.index, foo['DAILY_FLOW'], size=10, color=\"red\", \n",
    "         alpha=0.5, legend_label='{} pts'.format(len(foo)))\n",
    "# p.line(input_sig.index, input_sig['f_sig'], color='blue')\n",
    "p.line(input_sig.index, input_sig['DAILY_FLOW'], color='blue')\n",
    "# p.line()\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the summer baseflow\n",
    "\n",
    "Break up the May to November records by periods where it comes back to within X% of baseflow.  \n",
    "\n",
    "check durations of these periods, see how many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
